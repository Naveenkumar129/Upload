vvp operations
Reinstall the operator with SSL disabled:

    helm upgrade --install flink-controller-operator helm/flink-controller-operator --namespace flink-operator --set targetNamespace=flink-jobs --set replicas=1

Expose the operator using an external address:

    kubectl -n flink-operator expose service flink-operator --name=flink-operator-external --port=4444 --target-port=4444 --external-ip=$(minikube ip)

Get the list of deployments:

    docker run --rm -it nextbreakpoint/flinkctl:1.5.0 deployments list --host=$(minikube ip) | jq -r '.output' | jq

Get the list of clusters:

    docker run --rm -it nextbreakpoint/flinkctl:1.5.0 clusters list --host=$(minikube ip) | jq -r '.output' | jq

Get the list of jobs:
-----------

helm list myapp sta/chart2 --version  "0.1.2"
  helm status myapp --show-resources
  helm list --superseded
  helm get values myapp --revision ` 2
  helm history  myapp
  helm install myapp char1 --set service.nodePort=3210
  helm get manifest myapp --revision
  helm get manifest  releasename  --revision
  helm get all myapp
  helm get value
  hlm get hooks releasename 


kubectl auth can-i create deployments --namespace=small --as=$READER_USER_ID

kubectl auth can-i create deployments 

$ kubectl auth can-i create deployments
$ kubectl auth can-i create deployments --as bob
$ kubectl auth can-i create deployments --as bob --namespace developer


kubectl get pods -o wide --show-labels'
kubectl get services -o wide --show-labels'
kubectl get deployments -o wide --show-labels'
kubectl auth can-i --list

  kubectl auth
  192  kubectl auth can-i create pod
  193  kubectl auth can-i delete ns
  194  kubectl auth can-i -h
  
  kubectl create role developer --resource=pods --verb=create,list,get,update,delete --namespace=development
  
  kubectl --token=$JWT auth can-i --list
  
  helm list myapp sta/chart2 --version  "0.1.2"
  helm status myapp --show-resources
  helm list --superseded
  helm get values myapp --revision ` 2
  helm history  myapp
  helm install myapp char1 --set service.nodePort=3210
  helm get manifest myapp --revision
  helm get manifest  releasename  --revision
  helm get all myapp
  helm get value
  hlm get hooks releasename 
  
  
You can filter by Helm labels:
kubectl get deployments -n <namespace> -l "app.kubernetes.io/instance=<release-name>"

Or list all resources (including deployments) created by a Helm release:
helm get manifest <release-name> -n <namespace> | grep -A5 'kind: Deployment'


To see all associated resources:
kubectl get all -n <namespace> -l "app.kubernetes.io/instance=<release-name>"
  

Get Images via helm
helm get manifest <release-name> -n <namespace> | grep 'image:' | awk '{print $2}' | sort -u

For a Get Images specific Helm release:
kubectl get all -n <namespace> -l "app.kubernetes.io/instance=<release-name>" -o jsonpath="{..image}" | tr -s '[[:space:]]' '\n' | sort -u
-----

https://download.csdn.net/download/weixin_42774671/12558501?utm_medium=distribute.pc_relevant_download.none-task-download-2~default~BlogCommendFromBaidu~Rate-9-12558501-download-15009440.257%5Ev16%5Epc_dl_relevant_base1_b&depth_1-utm_source=distribute.pc_relevant_download.none-task-download-2~default~BlogCommendFromBaidu~Rate-9-12558501-download-15009440.257%5Ev16%5Epc_dl_relevant_base1_b&spm=1003.2020.3001.6616.10


https://download.csdn.net/download/xiluoenm/84230866?utm_medium=distribute.pc_relevant_download.none-task-download-2~default~OPENSEARCH~Rate-21-84230866-download-19260998.257%5Ev16%5Epc_dl_relevant_base1_b&depth_1-utm_source=distribute.pc_relevant_download.none-task-download-2~default~OPENSEARCH~Rate-21-84230866-download-19260998.257%5Ev16%5Epc_dl_relevant_base1_b&spm=1003.2020.3001.6616.21

To list deployment	:kubectl get deploy
To show addtional info	:kubectl get deploy -o wide
To show complete info	:kubectl describe deploy name-of-deployment
To delete the deploy	:kubectl delete deploy name-of-deploy
to get lables of pods 	:kubectl get pods -l app=swiggy
TO scale deploy		:kubectl scale deploy/name-of-deploy --replicas=10 (LIFO)
To edit deploy		:kubectl edit deploy/name-of-deploy
to show all pod labels	:kubectl get pods --show-labels
To delete all pods	:kubectl delete pod --all


If you want to temporarily pause the operator:
Scale down the operator deployment:

kubectl scale deployment ververica-operator --replicas=0 -n <operator-namespace>
Scale it back up when needed:


kubectl scale deployment ververica-operator --replicas=1 -n <operator-namespace>


kubectl Cp topics-to-process.ison $BROKER:/tmp -c kafka-broker
$K_EXEC "$KAFKA HOME/bin/kafka-reassign-partitions.sh I
--bootstrap-server $BOOT_ STRAP \
-command-confiq $KAFKA_HOME/config/producer.properties I
--broker-list 1,2,3,4,5,6 I
--topics-to-move-ison-file /tmp/topics-to-process.ison I
--generate I grer -Al 'Proposed partition reassignment configuration


VERIFY.tbt +


Generate NLP CERT PROD.td X


# JSONPATH

kubectl get nodes -o wide  -o=jsonpath='{.items[*].status.capacity.cpu}'
kubectl get nodes -o wide  -o=jsonpath='{.items[*].metadata.name}{"\n"}{.items[*].status.capacity.cpu}'
kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
kubectl get nodes --sort-by= .status.capacity.cpu

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}'
kubectl get nodes -o=jsonpath='{.items[*].status.nodeInfo.osImage}'

kubectl config view --kubeconfig=/root/my-kube-config


Rrintf "Ingenerated the following topics-reassignment.is0nln"
$K_EXEC "cat /tmp/topics-reassignment.13Qn"


printf "Iaxunning reassignment \n"
reassigning
$K EXEC "$KAFKA HOME/bin/ķafka-reassign-partitions.sh I
--bootstrap-server $BOOT STRAP
-command-config $KAFKA HOME/config/producer.properties I
-reassignment-ison-file /tmr/topics-reassignment.json
execute"
--


+


Rrintf "Iaverifying the reassignment in"
t verify
$K EXEC "$KAFKA HOME/bin/kafka-reassign-partitions.sh I
--bootstrap-server $BOOT STRAP
T
command-config $KAFKA_HOME/confiq/producer.properties I
--
-reassignment-ison-file /tmp/topics-reassignment.jsop
N
-verify"


H


44


printf "Inshesking the reassignmentin"
checking
$K EXEC "$KAFKA HOME/bin/kafka-reassign-partitions.sh I
-bootstrap-server $BOOT STRAP |
--command-confiq $KAFKA_HOME/confiq/producer.properties
--list"





# Stop services cleanly
/opt/IBM/InformationServer/ASBServer/bin/NodeAgents.sh stop

# Start them again with new cert and DB config
/opt/IBM/InformationServer/ASBServer/bin/NodeAgents.sh start
---------
Define Pipeline Variables
In the Harness pipeline:

replicas = 2

crds = false

retention = 7d

============

#####
some flink https://github.com/CloudLargeScale-UCLouvain/flink-active-replication/blob/1563724b1707ac3ab8e2655843346057b5c5327c/scripts/measure.py
##########

kubectl get pods | grep taskmanager | awk '{print $1}' | xargs -I {} sh -c 'kubectl logs $1 > ../measurements/$2/tm-$1-logs.txt' -- {} <run_date>

kubectl get pods | grep jobmanager | awk '{print $1}' | xargs -I {} kubectl logs {} > ../measurements/<run_date>/jm-logs.txt
Verify Cleanup:
kubectl get all -n flink
kubectl get crds | grep flink

Flink Kubernetes Operator	Manages Flink clusters and jobs in Kubernetes
CRDs (flinkdeployments, flinksessionjobs, flinkclusters)


openssl req -new -x509 -key existing-key.pem -out certificate.crt -days 365 \
    -subj "/C=US/ST=California/L=San Francisco/O=My Company/CN=example.com"


subprocess.check_call("kubectl scale deployment flink-jobmanager --replicas=0", shell=True)
    subprocess.check_call("kubectl scale deployment flink-taskmanager --replicas=0", shell=True)
    time.sleep(5)
    subprocess.check_call("kubectl scale deployment flink-jobmanager --replicas=1", shell=True)
    subprocess.check_call("kubectl scale deployment flink-taskmanager --replicas=30", shell=True)
    subprocess.check_call("kubectl wait --for=condition=ready pod -l app=flink-jobmanager --timeout=90s", shell=True)
    subprocess.check_call("kubectl wait --for=condition=ready pod -l app=flink-taskmanager --timeout=90s", shell=True)


--

Checkpoint / Savepoint Failures
Cause:
Slow or failing checkpoint storage (e.g., S3, HDFS).
Inconsistent state causing deserialization errors.
High checkpointing frequency causing excessive I/O.
Resolution:
✅ Verify checkpoint storage (state.checkpoints.dir).
✅ Reduce checkpointing frequency (execution.checkpointing.interval: 60s).
✅ Use incremental checkpoints (state.backend.incremental: true).
✅ Check logs for state deserialization errors.

5. Network Failures
Cause:
Flink JobManager and TaskManager communication issues.
Kubernetes/YARN network connectivity problems.
Resolution:
✅ Check Flink logs (jobmanager.log, taskmanager.log).
✅ Increase network retries (akka.ask.timeout: 30s).
✅ Verify Kubernetes networking (kubectl get pods -o wide).
✅ Tune network buffer settings (taskmanager.network.memory.*).

6. JobManager Failure
Cause:
High memory consumption in JobManager.
Checkpointing taking too long.
JobManager lost connection with TaskManagers.
Resolution:
✅ Restart JobManager (flink run ...).
✅ Enable high-availability mode (high-availability: zookeeper).
✅ Increase JobManager memory (jobmanager.memory.process.size).
✅ Check JobManager logs (kubectl logs <jobmanager-pod>).

7. Unexpected Serialization Issues
Cause:
Incompatible class versions between job upgrades.
Using non-serializable objects in operators.
Resolution:
✅ Ensure compatible state serialization (state.backend.rocksdb.enable-native = true).
✅ Use Flink’s TypeSerializer instead of Java serialization.
✅ Use savepoints for state migration.

8. Job Stuck in Restart Loop
Cause:
Job restart strategy triggers too frequently.
Persistent errors (e.g., state corruption, dependency issues).
Resolution:
✅ Check Flink restart strategy (restart-strategy: fixed-delay).
✅ Increase delay between restarts (restart-strategy.fixed-delay.delay: 10s).
✅ Verify dependencies (flink run --jar job.jar).





/*********** *******
JENKINS -KAFKA PIPELINE 
***********************/
all brokers present in cluster heath status
check the authorizations
publish message asyn to topic
publish message syn to topic

publish message with key
publish message with parttion number or id 
read topic using consumer group and consumer
produce and consumer on a topic;
poll the message from topic with partition id


TOPIC :

TOPIC Creation
TOPIC Deletion
parttion increase

USER:
User creation
user deletion

ACL:
Add permission to user
remove permission to user

Report :
List of topics
List of user with ACL
List of ACL fro specific user


Trigger compaction of topic
kafka-config

verify compaction on topic




/*********** *******
FLINK 

***********************/

Ensure JobManager pod (flink-jobmanager-0) is running.
Ensure TaskManager pods are running (flink-taskmanager-*).

kubectl logs -f <jobmanager-pod-name> -n <namespace>
kubectl logs -f <taskmanager-pod-name> -n <namespace>
kubectl get pods -n flink | grep taskmanager
kubectl get pods -n flink | grep jobmanager
kubectl get pods -n <namespace>	Check pod status
kubectl describe pod <pod-name> -n <namespace>	Inspect pod events
kubectl logs -f <jobmanager-pod-name> -n <namespace>	Debug JobManager
kubectl logs -f <taskmanager-pod-name> -n <namespace>	Debug TaskManager
kubectl port-forward svc/<flink-jobmanager-service> 8081:8081 -n <namespace>	Access Flink UI
helm list -n <namespace>	Check Helm deployment
kubectl rollout restart deployment flink-jobmanager -n <namespace>	Restart components

--------------------
EC "$SKAFKA_HOME/bin/kafka-configs.sh
-bootstrap-server $BOOT_STRAP
-command-config $KAFKA_HOME/config/consumer.properties
--alter


- entity-type topics
-entity-name $1
oe
--add-config min.cleanable.dirty.ratio=$2" I egrep -v



SK_EXEC "SKAFKA_HOME/bin/kafka-configs.sh I
--bootstrap-server $BOOT_STRAP \
-command-config $KAFKA_HOME/config/consumer.propert


-alter
-entity-type topics
-entity-name $1 \
add-config retention. ms=$2"
::::::
. setEnv.sh


file="topics.txt"
mapfile -t lines < "Sfile"
for line in "$(lines[@])"; do
echo "topic list: $line"
$K_EXEC "$KAFKA_HOME/bin/kafka-configs.sh I
-bootstrap-server $BOOT_STRAP

-command-config SKAFKA_HOME/config/consumer.prop


-alter
-topic "$line" \
-add-config min.insync.replicas=3"
done



goSororities new sink onnectarinesIts
kafka-topics in --1100--TOPIC SHIITEPI SHIII ProcessState Transformed BOOTSTRAP-Server KALKI-70001000 9092
#Ft Ötürüdüşünmüştün * şükrünü
Series- Not produces 28--topic Payload Reporting ATRACE-DAYIttap-outer kaffa-headless 5092
kafka-topics. 2--create topic Payload-Reporting-Ripeits bootstrap-server kills-headless 9092- partitions 3-- cells clearly policy deictic
Sorted tuneless as Lake tools ActorIs theIntolerance SOS? topic far Author Asp Bats
TOP-SEEDED katie headless 9092 -TOPIC PAY 0 K port no 7 11 REPEATS Off... 1--partition 0 lanne
kitts-console-consumer in --bootstrap-server kafka-headless 9092--topic Payload-Reporting-failed-RIPCATS from-beatenand
kitty console-producer at --topic & flood he often RIPCAT-bootstrap-server kafka-headless 5052
barks-run-class of kafka tools Gotoffsetshell-bentex-laxtake-atiers 9092--topic Rebuilding payment,
I of alt text gain are 093/connectors/ 1 and I post n I then It at sake connectorstatts I python In gear tool


>>>[[>[[>>>>[
metrics to view before rebalancing a partitions 
Before rebalancing partitions in Kafka, it is important to monitor several key metrics to ensure that the cluster is healthy and that the rebalancing
Wwill not cause any disruption to the ongoing data processing. Some of the key metrics to consider include:


Broker CPU and memory utilization: Rebalancing partitions may cause a temporary increase in CPU and memory utilization on the brokers.
these metrics to ensure that the brokers have sufficient resources to handle the increased load during the rebalancing process


You should monitor


increase in network traffic as the brokers exchange metadatą and data between each other. You
and the network infrastructure can handle the increased load during the rebalancing process


Network traffic: Rebalancing partitions may causeshould monitor the network traffic 
to ensure that a temporarythe brokers


Topic and partition lag: Topic and partition lag is an important metric to monitor in Kafka, as it indicates
and the time it is consumed. If there is a significant lag in a topic or partition, it may indicate that the
rate, and rebalancing partitions may not be effective in addressing the issue


the delay between the
consumer group is not


Consumer group lag: Consumer group lag is
metric that indicates the difference between the latest
processed by the consumer group. If there is a significant lag in a consumer group, it may indicate
partitions fast enough, and rebalancing partitions may not be effective in addressing the issue


offset of a partition and the offset of the last message
that the consumer group is not pulling data from the


Partition skew: Partition skew is a metric that indicates the imbalance of data distribution across partitions in a topic. If there is a significant
partition skew, it may indicate that some partitions are handling more data than others, which can lead to performance and availability issues. Rebalancing
help address partition skew and ensure a more even distribution of data across 


By monitoring these metrics before rebalancing partitions in Kafka,
cause any disruption to the ongoing data processing.


you can ensure that


the cluster is healthy and that the


rebalancing process will n

@#########
cat /var/log/kafka/server.log | grep "replication"     --->broker logs for slow fetches
kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions
bin/kafka-topics.sh --describe --topic <topic-name> --bootstrap-server <broker> | grep UnderReplicatedPartitions
bin/kafka-topics.sh --describe --topic <topic-name> --bootstrap-server <broker>
Optimize log.segment.bytes, log.retention.ms, and log.retention.bytes.



--------
kafka-configs.sh --bootstrap-server <broker-address> --entity-type brokers --entity-name <broker-id> --describe
kafka-configs.sh --bootstrap-server <broker-address> --entity-type brokers --entity-name <broker-id> --alter --add-config num.network.threads=8,num.io.threads=4"
kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name <topic-name> --alter --add-config "retention.ms=1000"
delete after 1 second

--add-config cleanup.policy=compact
--add-config  delete.retention.ms=10000
--add-config segment.ms=1000
-add-config min.cleanable.dirty.ratio=0.01
--add-config min.compact.lag.ms=500


/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name __consumer_offsets --alter --add-config "retention.ms=86400000"  # Retention for 1 day
/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name <topic-name> --alter --add-config  cleanup.policy=compact"

kafka-topics.sh --bootstrap-server localhost:9092 --create --topic employee-salary  --partitions 1 --replication-factor 1  --config cleanup.policy=compact  --config min.cleanable.dirty.ratio=0.001  --config segment.ms=5000


# cluster-wider:
$ bin/kafka-configs.sh 
  --bootstrap-server host:port 
  --entity-type brokers 
  --entity-default # cluster-wider
  --alter 
  --add-config unclean.leader.election.enable=true

# per-broker:
$ bin/kafka-configs.sh 
  --bootstrap-server host:port 
  --entity-type brokers 
  --entity-name 1 # per-broker
  --alter 
  --add-config unclean.leader.election.enable=false


bin/kafka-topics.sh --describe --topic __consumer_offsets --bootstrap-server localhost:9092 
bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 1 --alter --add-config "offsets.retention.minutes=20160"


--

message.max.bytes  Meaning: Maximum size of a message;
log.cleaner.enabled  Log compacted: For each key, only the latest message is kept
----

kafka-configs.sh --bootstrap-server <KAFKA_BOOTSTRAP_SERVER> \
  --alter --entity-type topics --entity-name $topic \
  --add-config retention.ms=-1


kafka-configs.sh --bootstrap-server <KAFKA_BOOTSTRAP_SERVER> \
  --alter --entity-type topics --entity-name $topic \
  --add-config cleanup.policy=compact,min.cleanable.dirty.ratio=0.1,segment.bytes=4194304


kafka-topics.sh --bootstrap-server <KAFKA_BOOTSTRAP_SERVER> \
  --describe --topic $topic


. Check Topic Retention Configuration
To view the current retention policy for a topic:


bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type topics --entity-name <topic-name>
Look for the retention.ms, retention.bytes, and cleanup.policy properties.

2. Set or Update Data Retention Policy
Modify the retention configuration for an existing topic:

a) Retention Based on Time (retention.ms)
Example: Keep data for 7 days (7 days * 24 hours * 60 minutes * 60 seconds * 1000 ms):

bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name <topic-name> --alter --add-config "retention.ms=604800000"

b) Retention Based on Size (retention.bytes)
Example: Limit the topic size to 1 GB:


bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name <topic-name> 
--alter --add-config "retention.bytes=1073741824"

c) Set Cleanup Policy (cleanup.policy)
Options: delete (default) or compact
Example to enable log compaction:

bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name <topic-name> --alter --add-config "cleanup.policy=compact"

3. Verify Updated Retention Settings
To confirm the changes:

bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type topics --entity-name <topic-name>
4. Broker-Level Retention Configurations (Global Settings)
You can set retention configurations globally in server.properties:

log.retention.ms=604800000      # Retention period (7 days)
log.retention.bytes=1073741824  # 1 GB per partition
log.segment.bytes=1073741824    # Segment file size
log.cleanup.policy=delete       # Cleanup policy
Restart the Kafka broker after making changes:

bin/kafka-server-start.sh config/server.properties


5. Remove Data from a Topic Manually
To delete all data from a topic:


bin/kafka-topics.sh --bootstrap-server localhost:9092 \
--delete --topic <topic-name>



----

Putty | mobaxterm >> Download

Kubernetes (K8S)
 
 sudo -i
 hostnamectl set-hostname master // same way set hostname as worker1 and worker2 for other 2 machines
 bash
 
 nano script.sh     // ctrl+o to save (ENTER) and ctrl+x  (ENTER) to exit
 
 ### Installation Script  ################
 create scriptfile vi script.sh
 
 
#!/bin/bash 
sudo apt update -y
sudo apt install apt-transport-https ca-certificates curl software-properties-common  -y
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable"
sudo apt update -y
apt-cache policy docker-ce -y
sudo apt install docker-ce -y
wget -q -O - https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
echo deb http://apt.kubernetes.io/ kubernetes-xenial main | sudo tee /etc/apt/sources.list.d/kubernetes.list
apt update -y
apt install kubelet=1.21.1-00 kubeadm=1.21.1-00 kubectl=1.21.1-00 -y
sysctl net.bridge.bridge-nf-call-iptables=1

################
 
sh script.sh  // to run script -- RUN on all machine

#RUN only on master

kubeadm init --pod-network-cidr=192.168.0.0/16 >> cluster_initialized.txt

tail -f cluster_initialized.txt
---------------

on master node from the above file and also shows the join command.

mkdir -p $HOME/.kube >> Create both .kube and its parent directory
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  >> copy configuration to kube directoy
  sudo chown $(id -u):$(id -g) $HOME/.kube/config >> provide user configuration to config file

kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml >> network

curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O

or 

kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml


====================
If not working then we have to reset kubeadm >>

kubeadm reset
rm -rf $HOME/.kube/config

=====================

  mkdir -p $HOME/.kube >> Create both .kube and its parent directory
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  >> copy configuration to kube directoy
  sudo chown $(id -u):$(id -g) $HOME/.kube/config >> provide user configuration to config file

  kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
  systemctl restart kubelet.service
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Paste join scripts on worker nodes

========================================================================================================================

if getting error >> The connection to the server localhost:8080 was refused 

either not using the correct user or follow below step :

kubeadm reset
rm -rf $HOME/.kube/config

=====================

  mkdir -p $HOME/.kube >> Create both .kube and its parent directory
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  >> copy configuration to kube directoy
  sudo chown $(id -u):$(id -g) $HOME/.kube/config >> provide user configuration to config file
=========================================================================================================================

important Files :

/etc/kubernetes/pki >> certificates
/etc/kubernetes/manifests >> static pods
/etc/kubernetes/ >> below config files where pki has all the certificates
admin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf

===================================================================================================
kubectl get pods -A -o wide
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE   IP                NODE              NOMINATED NODE   READINESS GATES
default       pardeep                                    1/1     Running   0          13m   192.168.84.2      sandeep-worker2   <none>           <none>
default       sandeep                                    1/1     Running   0          16m   192.168.184.129   sandeep-worker1   <none>           <none>
default       sumit                                      1/1     Running   0          13m   192.168.84.1      sandeep-worker2   <none>           <none>
kube-system   calico-kube-controllers-69f595f8f8-zzb52   1/1     Running   0          44m   192.168.141.129   sandeep-master    <none>           <none>
kube-system   calico-node-d6wp4                          1/1     Running   0          26m   172.31.21.253     sandeep-worker1   <none>           <none>
kube-system   calico-node-hdv8p                          1/1     Running   0          44m   172.31.28.185     sandeep-master    <none>           <none>
kube-system   calico-node-xhnq9                          1/1     Running   0          27m   172.31.20.178     sandeep-worker2   <none>           <none>
kube-system   coredns-558bd4d5db-67mz6                   1/1     Running   0          46m   192.168.141.130   sandeep-master    <none>           <none>
kube-system   coredns-558bd4d5db-z6j2x                   1/1     Running   0          46m   192.168.141.131   sandeep-master    <none>           <none>
kube-system   etcd-sandeep-master                        1/1     Running   0          47m   172.31.28.185     sandeep-master    <none>           <none>
kube-system   kube-apiserver-sandeep-master              1/1     Running   0          47m   172.31.28.185     sandeep-master    <none>           <none>
kube-system   kube-controller-manager-sandeep-master     1/1     Running   0          47m   172.31.28.185     sandeep-master    <none>           <none>
kube-system   kube-proxy-d5nh5                           1/1     Running   0          46m   172.31.28.185     sandeep-master    <none>           <none>
kube-system   kube-proxy-l9znx                           1/1     Running   0          26m   172.31.21.253     sandeep-worker1   <none>           <none>
kube-system   kube-proxy-qd5hk                           1/1     Running   0          27m   172.31.20.178     sandeep-worker2   <none>           <none>
kube-system   kube-scheduler-sandeep-master              1/1     Running   0          47m   172.31.28.185     sandeep-master    <none>           <none>


==================================================================================================


then 
kubectl get nodes
kubectl label node sandeep-worker1 node-role.kubernetes.io/worker=worker   >> to set the role to the worker node
kubectl get pods
kubectl get pods -A
kubectl get pods -A | grep -i worker1

### Run on worker1 ########
docker ps and docker rm to remove a container...

#################################
cat /root/.kube/config

======================================

kubectl get pods -A
ls -l /root/.kube/config
docker ps
ps -ef
docker ps   >> will show all the pods including the manager pod
cd /
clear
kubectl get pods
kubectl delete pod pod1
clear
kubectl run gagan-app --image httpd
kubectl get pods
kubectl get pods -o wide
curl 192.168.235.130
kubectl describe pod gagan-app | more
kubectl get pods
kubectl logs gagan-app
kubectl exec -it gagan-app -- /bin/bash
        to install curl and ping in the container for testing
        apt update -y
        apt-get install curl iputils-ping -y
==================================================

kubectl create deployment myapp1 --image httpd --replicas=2
kubectl get pods
kubectl get pods -o wide
kubectl get deployments
kubectl describe deploy myapp1
kubectl get pods
kubectl delete pod myapp1-7d8bc764bd-nhkwr
kubectl get pods
kubectl scale deployment myapp1 --replicas=4
kubectl get pods
kubectl get pods -o wide
kubectl scale deployment myapp1 --replicas=1
kubectl get pods -o wide
kubectl delete pod myapp1-7d8bc764bd-szbsm
kubectl get pods -o wide
kubectl delete deploy myapp1
kubectl get pods -o wide

==================================================

kubectl create deployment myapp1 --image httpd:2.4 --replicas=4
kubectl get deployment
kubectl describe deploy myapp1
kubectl get rs
kubectl get replicaset
kubectl describe rs myapp1-744c9889bd
kubectl get pods
kubectl delete pod myapp1-744c9889bd-tn8ll
kubectl get pods
kubectl describe rs myapp1-744c9889bd
kubectl get pods
kubectl describe pod myapp1-744c9889bd-fprnq
kubectl get all
kubectl delete rs myapp1-744c9889bd
kubectl get all
kubectl delete deployment myapp1
kubectl get all

===================================================
telecom-app:v1, v2, v3

deployment telecom-app
rs -- v1, v2, v3
pods - pod1:v2   pod2:v2   pod3:v2


vi pod.yml

apiVersion: v1
Kind: Pod
Metadata:
  Name: sandy-pod
Spec:
  Containers:
    - name: container1
      Image: httpd









 1  clear
    2  vi script.sh
    3  sh script.sh
    4  kubeadm init --pod-network-cidr=10.10.0.0/16 >> cluster_initialized.txt
    5  ls
    6  cat cluster_initialized.txt
    7  mkdir -p $HOME/.kube
    8  cat $HOME/.kube/config
    9  kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml
   10  kubectl get nodes
   11  kubectl get nodes -w
   12  kubectl get nodes
   13  kubectl get nodes -w
   14  kubectl get nodes
   15  kubectl get pods
   16  kubectl get ns
   17  kubectl get pods -n kube-system
   18  kubectl get pods -n kube-system -o wide
   19  kubectl run pod1 --image nginx
   20  kubectl get pods
   21  kubectl describe pod pod1
   22  kubectl get pods -o wide
   23  docker ps
   24  clear
   25  kubectl get pods
   26  kubectl get pods -o wide
   27  kubectl get pods -n kube-system
   28  kubectl get pods -all
   29  kubectl get pods --all
   30  kubectl get pods -a
   31  kubectl get pods -A
   32  kubectl get pods -A -o wide
   33  docker ps
   34  clear
   35  cd /etc/kubernetes/
   36  ls
   37  cd manifests/
   38  ls
   39  kubectl get pods
   40  kubectl get pods -o wide
   41  kubectl get pods -A
   42  kubectl create deploy frontend --image nginx --replicas=4 --dry-run=client -o yaml > deploy.yaml
   43  vi deploy.yaml
   44  kubectl apply -f deploy.yaml
   45  kubectl get pods
   46  kubectl get pods -o wide
   47  kubectl get svc
   48  history

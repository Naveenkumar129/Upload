
curl -X POST -H "  Authorization: Bearer $MCPGATEWAY_BEARER_TOKEN" \
	-H "  Content-Type: application/json" \
	-d ' {
		 "name" : "clock_tool" ,
		 "url" : "http://localhost:9000/rpc" ,
		 "description" : "Returns current time" ,
		 "input_schema" : {
			 "type" : "object" ,
			 "properties" : { "timezone" : { "type" : "string" }}
		}
	}' \
	http: //localhost:4444/tools





-----------
Describe Topics with Replica Info
kafka-topics.sh --bootstrap-server <broker> --describe --topic <topic>


Example output:

Topic: my-topic   PartitionCount: 3  ReplicationFactor: 2  Configs:
    Partition: 0  Leader: 101  Replicas: 101,102  Isr: 101,102
    Partition: 1  Leader: 102  Replicas: 102,103  Isr: 102,103
    Partition: 2  Leader: 103  Replicas: 103,101  Isr: 103,101


Here:

Replicas = which brokers host replicas (placement)

Leader = active leader broker

ISR = in-sync replicas

üîπ 2. Export All Topics‚Äô Replica Placement

To dump replica placement for all topics:

kafka-topics.sh --bootstrap-server <broker> --describe > topics_rpp.txt

üîπ 3. Extract Only Replica Placement (Cleaner Output)
kafka-topics.sh --bootstrap-server <broker> --describe | \
awk '/Topic:/ {topic=$2} /Partition:/ {print topic, "Partition=" $2, "Leader=" $4, "Replicas=" $6, "ISR=" $8}'


Sample output:

my-topic Partition=0 Leader=101 Replicas=101,102 ISR=101,102
my-topic Partition=1 Leader=102 Replicas=102,103 ISR=102,103
my-topic Partition=2 Leader=103 Replicas=103,101 ISR=103,101

üîπ 4. Export to CSV
echo "Topic,Partition,Leader,Replicas,ISR" > topics_rpp.csv
kafka-topics.sh --bootstrap-server <broker> --describe | \
awk '/Topic:/ {topic=$2} /Partition:/ {print topic","$2","$4","$6","$8}' >> topics_rpp.csv


This gives a structured CSV with topic name, partition, leader, replicas, and ISR ‚Üí effectively the replica placement policy.

‚úÖ This will give you a full export of replica placement for every topic/partition.

Do you want me to extend this into a ready-to-run shell script that you can just run once and it will generate the CSV for your whole cluster?










kannika

kubectl get rolebinding,clusterrolebinding --all-namespaces -o json \
  | jq -r '.items[] | select(.subjects[]?.name=="my-app-sa") | .metadata.name + " ‚Üí " + .roleRef.kind + "/" + .roleRef.name'






Kannika API only needs the public key (from JWKS) to verify tokens.

curl -X POST https://api.kannika.example.com/gql \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"query":"mutation { createBackup(input:{name:\"daily\"}) { id status } }"}'

his is exactly what Kannika API does internally when it receives a Bearer token.

The API checks the token signature against the OIDC provider‚Äôs public keys.
----
TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)
curl -k -H "Authorization: Bearer $TOKEN" https://kubernetes.default/api/v1/
# gets a list of API resources
curl -k -H "Authorization: Bearer $TOKEN" https://kubernetes.default/api/v1/namespaces/default/pods


curl -k "https://${my_api_server}:6443/version";
curl -k "https://${my_api_server}:6443/api";
curl -k "https://${my_api_server}:6443/api/v1";
curl -k "https://${my_api_server}:6443/api/v1/namespaces";
curl -k "https://${my_api_server}:6443/api/v1/namespaces/<namespace>/pods";
curl -k "https://${my_api_server}:6443/apis";
-----------

curl -X POST http://<kannika-api-host>/api/v1/backup \
  -H "Content-Type: application/json" \
  -d '{
        "cluster": "kafka-cluster-1",
        "backupName": "backup-20250820",
        "topics": ["orders", "payments", "transactions"],
        "storageLocation": "s3://my-bucket/kafka-backups"
      }'

--topics-regex  TOPICS_REGEX
---

if regex 

"topicsRegex": ".*orders.*|payments-.*"
---
curl -X POST http://<kannika-api-host>/api/v1/restore \
  -H "Content-Type: application/json" \
  -d '{
        "cluster": "kafka-cluster-1",
        "backupName": "backup-20250820",
        "topicsRegex": "transactions-.*",
        "targetCluster": "kafka-cluster-2"
      }'

----


---
Step 1: Define a Backup CRD (example)
apiVersion: mybackup.io/v1alpha1
kind: Backup
metadata:
  name: kafka-topic-backup
  namespace: backup-system
spec:
  target:
    kind: KafkaTopic
    name: my-topic
  storage:
    type: s3
    bucket: kafka-backups
    prefix: my-topic


This represents the backup resource. (In real-world setups, a controller/operator like Velero or Kanister would reconcile this and actually perform the backup.)

üîÑ Step 2: Create a Job for backup
apiVersion: batch/v1
kind: Job
metadata:
  name: kafka-backup-job
  namespace: backup-system
spec:
  template:
    spec:
      containers:
        - name: backup
          image: mybackup-tool:latest
          command: ["backup.sh"]
          args: ["--target=my-topic", "--bucket=kafka-backups"]
      restartPolicy: OnFailure


This will run the backup one time.

‚è≤Ô∏è Step 3: Schedule backups using CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: kafka-backup-schedule
  namespace: backup-system
spec:
  schedule: "0 2 * * *"   # Every day at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: backup
              image: mybackup-tool:latest
              command: ["backup.sh"]
              args: ["--target=my-topic", "--bucket=kafka-backups"]
          restartPolicy: OnFailure

üîÑ Step 4: Restore Job
apiVersion: batch/v1
kind: Job
metadata:
  name: kafka-restore-job
  namespace: backup-system
spec:
  template:
    spec:
      containers:
        - name: restore
          image: mybackup-tool:latest
          command: ["restore.sh"]
          args: ["--target=my-topic", "--bucket=kafka-backups", "--snapshot=latest"]
      restartPolicy: OnFailure


‚úÖ Flow:

CronJob ‚Üí schedules backup jobs.

Backup stored in S3/minio/etc.

Restore job ‚Üí recreates data/configs when needed.

If you‚Äôre using Velero/ Kanister, the CRDs (Backup, Restore) are already provided.

üëâ Naveen, do you want me to give you a Velero-based real example (with Backup and Restore kinds) instead of this generic simulation? That way you can actually apply and run it on your kind cluster.

ChatGPT can make mistakes. Check important






Prepare Before Speaking

Note specific instances with date, time, and impact on your work.

Be ready with examples where interruptions, extended calls, or after-hours messages delayed deliverables or caused errors.

Frame your concern as a productivity and work quality issue, not a personal criticism.

2. Use a Professional Conversation Structure

a. Start with appreciation

‚ÄúI appreciate how actively you‚Äôre involved and how you want things on track. It shows you care about the outcome.‚Äù

b. State the impact (not the person)

‚ÄúLately, I‚Äôve noticed BAU calls running beyond the scheduled time and frequent desk visits/calls for updates. While I understand the intent, these frequent interruptions make it harder for me to stay focused and complete tasks efficiently.‚Äù

c. Show consequences

‚ÄúSometimes, when I‚Äôm mid-task and have to switch context repeatedly, it slows progress and increases the chance of mistakes. It also extends my work hours.‚Äù

d. Propose a constructive solution

‚ÄúWould it help if we had a fixed daily/weekly update slot for status, and I send a short written update in between? This way, you still have visibility, and I can keep focus on delivering work faster.‚Äù

e. Address after-hours calls politely

‚ÄúIf something is urgent, I‚Äôm happy to address it after hours, but for non-critical items, could we keep them for the next day‚Äôs sync? That helps me maintain a balance while still keeping priorities in check.‚Äù

3. Keep Your Tone Neutral

Avoid ‚ÄúYou always‚Ä¶‚Äù or ‚ÄúYou don‚Äôt let me‚Ä¶‚Äù ‚Üí Instead, use ‚ÄúI‚Äôm finding it challenging when‚Ä¶‚Äù

Stay calm even if they push back.

Keep the discussion about workflow, not personality.

4. Follow Up in Writing
If this is a formal workplace, you can also send a short, polite email summarizing the agreed points after your discussion. This makes it easier to refer back if the behavior continues.

If you‚Äôd like, I can draft you a ready-to-use short email that makes this point firmly but respectfully so it doesn‚Äôt sound like you‚Äôre complaining. This way, it‚Äôs on record but still professional.

Do you want me to prepare that email?



My Technical Contributions
Over the past months, I have been deeply involved in the following areas:

Kafka, Cluster Linking, VVP, and DataStage product suites.

Recent hands-on work with the Kannika product and IBM MQ-based tasks as per JIRA assignments.

Proactively engaging with Confluent for Kubernetes and related tools that I highlighted as bold focus areas in last year‚Äôs survey.

I also value the emphasis you've placed on data center backup and restore, and I fully understand its criticality in the OCC (Operational Control Center) context. I make it a point to align my practices with this direction.

These are areas where I feel confident and contribute with an expert-level understanding.

üß≠ Concerns with Project Management (ABC)
While I remain committed to delivering high-quality results, I‚Äôd like to share some concerns regarding the current project manager, ABC, which have been consistently affecting team morale and productivity:

Redundant Questioning: Key use cases (e.g., VVP workspace creation) are asked repeatedly even after being clearly explained. This has happened multiple times with different teammates.

Incorrect Technical Directions: ABC occasionally provides incorrect implementation suggestions for production workflows, which could lead to operational risks.

Inefficient Meetings: Our daily meetings often stretch 45 minutes to an hour without clear outcomes, mainly due to repetitive discussions led by ABC.

Lack of Recognition: Technical input shared by me is often passed to management without acknowledgment, which feels demotivating.

Follow-through Gaps: Important technical discussions often do not result in action or proper documentation.

Confidentiality Breach: I‚Äôve experienced performance-related discussions being shared internally, which impacted me personally and did not reflect a professional standard.

Low Ownership During Pressure: Unlike many managers who take up small JIRA tasks or assist technically, ABC often remains distant even when the team is under significant workload.

üôè Intent and Outlook
I am sharing this feedback not with the intent of blame, but to support a more accountable, collaborative, and technically sound environment‚Äîone that aligns with the high standards we all strive for.

I‚Äôd appreciate your guidance on how best to handle this moving forward or if a more formal channel is preferred for such feedback.

Thank you for your attention and leadership.

Warm regards,
[Your Full Name]
[Your Job Title or Department]
[Your Contact Information]

Let me know if you'd like this tailored to a more informal tone, or if you're sending it via an internal platform (like Outlook, Slack, etc.).


============
Subject: Constructive Feedback Regarding Project Management Challenges

Dear [Executive Director's Name],

I hope this message finds you well.

I would like to bring to your attention some ongoing concerns regarding the project management style and approach of [Project Manager ABC], which I believe are impacting team productivity and morale.

Over the past months, I have been actively contributing to multiple high-impact areas, including Kafka, Cluster Linking, VVP, Datastage, and more recently Kannika and MQ products, handling multiple Jira tasks. I feel confident and technically sound in these domains.

However, I‚Äôve observed the following challenges while working under ABC‚Äôs coordination:

Repetitive Questions & Inefficient Communication:
Important use cases and product scenarios (e.g., VVP workspace creation) are often re-discussed unnecessarily in multiple meetings. The same questions are asked again with other team members, leading to confusion and redundancy.

Incorrect Technical Inputs:
At times, ABC provides technical suggestions that are incorrect or misaligned, which have been directed toward production implementation. This creates risks in sensitive environments.

Unproductive Meetings:
Daily meetings often extend up to 45 minutes to an hour with repetitive discussions, reducing valuable development and analysis time for the team.

Technical Input Not Acknowledged:
While I frequently provide technical feedback, it is often repackaged and forwarded to higher management without clarity or appropriate action being taken.

Lack of Follow-Through:
Several technical discussions end without action or follow-up from ABC, which impacts our delivery timelines and accountability.

Concerns with Confidentiality and Professionalism:
On one occasion, I experienced discomfort when personal performance-related concerns were shared within the team, breaching professional confidentiality norms.

Limited Team Support in High-Pressure Times:
Unlike other managers, ABC does not engage in Jira resolution or contribute technically, even during critical project phases when the team is under significant pressure.

I am sharing this as constructive feedback with the hope that it will help foster a more effective and professional working environment. My intention is to ensure that the team functions smoothly and delivers high-quality outcomes, and I remain committed to contributing with my full capabilities.

Thank you for your time and understanding. I would appreciate the opportunity to discuss this further if needed.


----
List all current aliases (verbose):
keytool -v -list -keystore truststore.p12 -storetype PKCS12 -storepass password

Check if desired alias is already used:
keytool -list -keystore truststore.p12 -alias <alias> -storepass password

If alias exists and you want a clean import, delete it first:
keytool -delete -alias <alias> -keystore truststore.p12 -storepass password

Import the new certificate:
keytool -importcert \
  -alias <alias> -file newcert.crt \
  -keystore truststore.p12 -storetype PKCS12 -storepass password
Or to overwrite without prompt:


keytool -importcert ‚Ä¶ -noprompt
üßæ

----
Important Considerations
Verify existing entries before importing:

bash
Copy code
keytool -list -keystore /apps/DataStage/jdk/jre/lib/security/cacerts-ts.jks \
  -storetype JKS -storepass cit
Backup your keystore file before making changes.

Confirm certificate file format: .cer must be in DER or PEM format accepted by Java Keytool.

Understand truststore locations in IBM InfoSphere:

Common paths include:

/opt/IBM/InformationServer/ASBServer/conf/iis-client-truststore.p12

/opt/IBM/InformationServer/ASBServer/conf/isf-node.keystore 
scribd.com
+2
ibm.com
+2
ibm.com
+2
stackoverflow.com
+1
manuallib.com
+1
scribd.com
+3
pdfcoffee.com
+3
scribd.com
+3
scribd.com

You may need to import to those depending on DataStage tier.

Restart services (e.g., DataStage engine, ASBServer, WebSphere) after import to apply the updated certificate.

‚úÖ Step-by-Step Renewal Process
Backup your existing truststore:

bash
Copy code
cp /apps/DataStage/jdk/jre/lib/security/cacerts-ts.jks \
   /apps/DataStage/jdk/jre/lib/security/cacerts-ts.jks.bak
Optionally list existing certificates:

bash
Copy code
keytool -list -keystore /apps/DataStage/jdk/jre/lib/security/cacerts-ts.jks \
  -storetype JKS -storepass cit
Import the new certificate (using the corrected command above).

Re-verify the alias appeared:

bash
Copy code
keytool -list -keystore /apps/DataStage/jdk/jre/lib/security/cacerts-ts.jks \
  -storetype JKS -storepass cit | grep db-ssl-dutla55l
Restart relevant DataStage components to ensure they load the updated keystore.

üìå Final Notes
Ensure the truststore you're updating is the one used by your JDBC or ASBServer SSL configurations.

If your environment uses PKCS12 files (e.g., .p12), substitute -storetype PKCS12 and password accordingly.

Always use consistent, absolute paths and verify current configurations before making changes.

Let me know if you want help verifying which truststore DataStage is actually using in your setup!











Sources








*******
vvp operations
Reinstall the operator with SSL disabled:

    helm upgrade --install flink-controller-operator helm/flink-controller-operator --namespace flink-operator --set targetNamespace=flink-jobs --set replicas=1

Expose the operator using an external address:

    kubectl -n flink-operator expose service flink-operator --name=flink-operator-external --port=4444 --target-port=4444 --external-ip=$(minikube ip)

Get the list of deployments:

    docker run --rm -it nextbreakpoint/flinkctl:1.5.0 deployments list --host=$(minikube ip) | jq -r '.output' | jq

Get the list of clusters:

    docker run --rm -it nextbreakpoint/flinkctl:1.5.0 clusters list --host=$(minikube ip) | jq -r '.output' | jq

Get list of jobs:

    curl http://localhost:4444/api/v1/clusters/custer-1/jobs

Get status of a deployment:

    curl http://localhost:4444/api/v1/deployments/example/status

Get status of a cluster:

    curl http://localhost:4444/api/v1/clusters/example/status

Get status of a job:

    curl http://localhost:4444/api/v1/clusters/example/jobs/measurement/status

Get details of a job:

    curl http://localhost:4444/api/v1/clusters/example/jobs/measurement/details

Get metrics of a job:

    curl http://localhost:4444/api/v1/clusters/example/jobs/measurement/metrics

Get metrics of the JobManager:

    curl http://localhost:4444/api/v1/clusters/example/jobmanager/metrics


Get the list of jobs:
-----------
helm commands :

helm list myapp sta/chart2 --version  "0.1.2"
  helm status myapp --show-resources
  helm list --superseded
  helm get values myapp --revision ` 2
  helm history  myapp
  helm install myapp char1 --set service.nodePort=3210
  helm get manifest myapp --revision
  helm get manifest  releasename  --revision
  helm get all myapp
  helm get value
  hlm get hooks releasename 


kubectl auth can-i create deployments --namespace=small --as=$READER_USER_ID

kubectl auth can-i create deployments 

$ kubectl auth can-i create deployments
$ kubectl auth can-i create deployments --as bob
$ kubectl auth can-i create deployments --as bob --namespace developer


kubectl get pods -o wide --show-labels'
kubectl get services -o wide --show-labels'
kubectl get deployments -o wide --show-labels'
kubectl auth can-i --list

  kubectl auth
  192  kubectl auth can-i create pod
  193  kubectl auth can-i delete ns
  194  kubectl auth can-i -h
  
  kubectl create role developer --resource=pods --verb=create,list,get,update,delete --namespace=development
  
  kubectl --token=$JWT auth can-i --list
  
  helm list myapp sta/chart2 --version  "0.1.2"
  helm status myapp --show-resources
  helm list --superseded
  helm get values myapp --revision ` 2
  helm history  myapp
  helm install myapp char1 --set service.nodePort=3210
  helm get manifest myapp --revision
  helm get manifest  releasename  --revision
  helm get all myapp
  helm get value
  hlm get hooks releasename 
  
  
You can filter by Helm labels:
kubectl get deployments -n <namespace> -l "app.kubernetes.io/instance=<release-name>"

Or list all resources (including deployments) created by a Helm release:
helm get manifest <release-name> -n <namespace> | grep -A5 'kind: Deployment'


To see all associated resources:
kubectl get all -n <namespace> -l "app.kubernetes.io/instance=<release-name>"
  

Get Images via helm
helm get manifest <release-name> -n <namespace> | grep 'image:' | awk '{print $2}' | sort -u

For a Get Images specific Helm release:
kubectl get all -n <namespace> -l "app.kubernetes.io/instance=<release-name>" -o jsonpath="{..image}" | tr -s '[[:space:]]' '\n' | sort -u
-----

https://download.csdn.net/download/weixin_42774671/12558501?utm_medium=distribute.pc_relevant_download.none-task-download-2~default~BlogCommendFromBaidu~Rate-9-12558501-download-15009440.257%5Ev16%5Epc_dl_relevant_base1_b&depth_1-utm_source=distribute.pc_relevant_download.none-task-download-2~default~BlogCommendFromBaidu~Rate-9-12558501-download-15009440.257%5Ev16%5Epc_dl_relevant_base1_b&spm=1003.2020.3001.6616.10


https://download.csdn.net/download/xiluoenm/84230866?utm_medium=distribute.pc_relevant_download.none-task-download-2~default~OPENSEARCH~Rate-21-84230866-download-19260998.257%5Ev16%5Epc_dl_relevant_base1_b&depth_1-utm_source=distribute.pc_relevant_download.none-task-download-2~default~OPENSEARCH~Rate-21-84230866-download-19260998.257%5Ev16%5Epc_dl_relevant_base1_b&spm=1003.2020.3001.6616.21

To list deployment	:kubectl get deploy
To show addtional info	:kubectl get deploy -o wide
To show complete info	:kubectl describe deploy name-of-deployment
To delete the deploy	:kubectl delete deploy name-of-deploy
to get lables of pods 	:kubectl get pods -l app=swiggy
TO scale deploy		:kubectl scale deploy/name-of-deploy --replicas=10 (LIFO)
To edit deploy		:kubectl edit deploy/name-of-deploy
to show all pod labels	:kubectl get pods --show-labels
To delete all pods	:kubectl delete pod --all


If you want to temporarily pause the operator:
Scale down the operator deployment:

kubectl scale deployment ververica-operator --replicas=0 -n <operator-namespace>
Scale it back up when needed:


kubectl scale deployment ververica-operator --replicas=1 -n <operator-namespace>


kubectl Cp topics-to-process.ison $BROKER:/tmp -c kafka-broker
$K_EXEC "$KAFKA HOME/bin/kafka-reassign-partitions.sh I
--bootstrap-server $BOOT_ STRAP \
-command-confiq $KAFKA_HOME/config/producer.properties I
--broker-list 1,2,3,4,5,6 I
--topics-to-move-ison-file /tmp/topics-to-process.ison I
--generate I grer -Al 'Proposed partition reassignment configuration


VERIFY.tbt +


Generate NLP CERT PROD.td X


# JSONPATH

kubectl get nodes -o wide  -o=jsonpath='{.items[*].status.capacity.cpu}'
kubectl get nodes -o wide  -o=jsonpath='{.items[*].metadata.name}{"\n"}{.items[*].status.capacity.cpu}'
kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
kubectl get nodes --sort-by= .status.capacity.cpu

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}'
kubectl get nodes -o=jsonpath='{.items[*].status.nodeInfo.osImage}'

kubectl config view --kubeconfig=/root/my-kube-config


Rrintf "Ingenerated the following topics-reassignment.is0nln"
$K_EXEC "cat /tmp/topics-reassignment.13Qn"


printf "Iaxunning reassignment \n"
reassigning
$K EXEC "$KAFKA HOME/bin/ƒ∑afka-reassign-partitions.sh I
--bootstrap-server $BOOT STRAP
-command-config $KAFKA HOME/config/producer.properties I
-reassignment-ison-file /tmr/topics-reassignment.json
execute"
--


+


Rrintf "Iaverifying the reassignment in"
t verify
$K EXEC "$KAFKA HOME/bin/kafka-reassign-partitions.sh I
--bootstrap-server $BOOT STRAP
T
command-config $KAFKA_HOME/confiq/producer.properties I
--
-reassignment-ison-file /tmp/topics-reassignment.jsop
N
-verify"


H


44


printf "Inshesking the reassignmentin"
checking
$K EXEC "$KAFKA HOME/bin/kafka-reassign-partitions.sh I
-bootstrap-server $BOOT STRAP |
--command-confiq $KAFKA_HOME/confiq/producer.properties
--list"





# Stop services cleanly
/opt/IBM/InformationServer/ASBServer/bin/NodeAgents.sh stop

# Start them again with new cert and DB config
/opt/IBM/InformationServer/ASBServer/bin/NodeAgents.sh start
---------
Define Pipeline Variables
In the Harness pipeline:

replicas = 2

crds = false

retention = 7d

============

#####
some flink https://github.com/CloudLargeScale-UCLouvain/flink-active-replication/blob/1563724b1707ac3ab8e2655843346057b5c5327c/scripts/measure.py
##########

kubectl get pods | grep taskmanager | awk '{print $1}' | xargs -I {} sh -c 'kubectl logs $1 > ../measurements/$2/tm-$1-logs.txt' -- {} <run_date>

kubectl get pods | grep jobmanager | awk '{print $1}' | xargs -I {} kubectl logs {} > ../measurements/<run_date>/jm-logs.txt
Verify Cleanup:
kubectl get all -n flink
kubectl get crds | grep flink

Flink Kubernetes Operator	Manages Flink clusters and jobs in Kubernetes
CRDs (flinkdeployments, flinksessionjobs, flinkclusters)


openssl req -new -x509 -key existing-key.pem -out certificate.crt -days 365 \
    -subj "/C=US/ST=California/L=San Francisco/O=My Company/CN=example.com"


subprocess.check_call("kubectl scale deployment flink-jobmanager --replicas=0", shell=True)
    subprocess.check_call("kubectl scale deployment flink-taskmanager --replicas=0", shell=True)
    time.sleep(5)
    subprocess.check_call("kubectl scale deployment flink-jobmanager --replicas=1", shell=True)
    subprocess.check_call("kubectl scale deployment flink-taskmanager --replicas=30", shell=True)
    subprocess.check_call("kubectl wait --for=condition=ready pod -l app=flink-jobmanager --timeout=90s", shell=True)
    subprocess.check_call("kubectl wait --for=condition=ready pod -l app=flink-taskmanager --timeout=90s", shell=True)


--

Checkpoint / Savepoint Failures
Cause:
Slow or failing checkpoint storage (e.g., S3, HDFS).
Inconsistent state causing deserialization errors.
High checkpointing frequency causing excessive I/O.
Resolution:
‚úÖ Verify checkpoint storage (state.checkpoints.dir).
‚úÖ Reduce checkpointing frequency (execution.checkpointing.interval: 60s).
‚úÖ Use incremental checkpoints (state.backend.incremental: true).
‚úÖ Check logs for state deserialization errors.

5. Network Failures
Cause:
Flink JobManager and TaskManager communication issues.
Kubernetes/YARN network connectivity problems.
Resolution:
‚úÖ Check Flink logs (jobmanager.log, taskmanager.log).
‚úÖ Increase network retries (akka.ask.timeout: 30s).
‚úÖ Verify Kubernetes networking (kubectl get pods -o wide).
‚úÖ Tune network buffer settings (taskmanager.network.memory.*).

6. JobManager Failure
Cause:
High memory consumption in JobManager.
Checkpointing taking too long.
JobManager lost connection with TaskManagers.
Resolution:
‚úÖ Restart JobManager (flink run ...).
‚úÖ Enable high-availability mode (high-availability: zookeeper).
‚úÖ Increase JobManager memory (jobmanager.memory.process.size).
‚úÖ Check JobManager logs (kubectl logs <jobmanager-pod>).

7. Unexpected Serialization Issues
Cause:
Incompatible class versions between job upgrades.
Using non-serializable objects in operators.
Resolution:
‚úÖ Ensure compatible state serialization (state.backend.rocksdb.enable-native = true).
‚úÖ Use Flink‚Äôs TypeSerializer instead of Java serialization.
‚úÖ Use savepoints for state migration.

8. Job Stuck in Restart Loop
Cause:
Job restart strategy triggers too frequently.
Persistent errors (e.g., state corruption, dependency issues).
Resolution:
‚úÖ Check Flink restart strategy (restart-strategy: fixed-delay).
‚úÖ Increase delay between restarts (restart-strategy.fixed-delay.delay: 10s).
‚úÖ Verify dependencies (flink run --jar job.jar).





/*********** *******
JENKINS -KAFKA PIPELINE 
***********************/
all brokers present in cluster heath status
check the authorizations
publish message asyn to topic
publish message syn to topic

publish message with key
publish message with parttion number or id 
read topic using consumer group and consumer
produce and consumer on a topic;
poll the message from topic with partition id


TOPIC :

TOPIC Creation
TOPIC Deletion
parttion increase

USER:
User creation
user deletion

ACL:
Add permission to user
remove permission to user

Report :
List of topics
List of user with ACL
List of ACL fro specific user


Trigger compaction of topic
kafka-config

verify compaction on topic




/*********** *******
FLINK 

***********************/

Ensure JobManager pod (flink-jobmanager-0) is running.
Ensure TaskManager pods are running (flink-taskmanager-*).

kubectl logs -f <jobmanager-pod-name> -n <namespace>
kubectl logs -f <taskmanager-pod-name> -n <namespace>
kubectl get pods -n flink | grep taskmanager
kubectl get pods -n flink | grep jobmanager
kubectl get pods -n <namespace>	Check pod status
kubectl describe pod <pod-name> -n <namespace>	Inspect pod events
kubectl logs -f <jobmanager-pod-name> -n <namespace>	Debug JobManager
kubectl logs -f <taskmanager-pod-name> -n <namespace>	Debug TaskManager
kubectl port-forward svc/<flink-jobmanager-service> 8081:8081 -n <namespace>	Access Flink UI
helm list -n <namespace>	Check Helm deployment
kubectl rollout restart deployment flink-jobmanager -n <namespace>	Restart components

--------------------
EC "$SKAFKA_HOME/bin/kafka-configs.sh
-bootstrap-server $BOOT_STRAP
-command-config $KAFKA_HOME/config/consumer.properties
--alter


- entity-type topics
-entity-name $1
oe
--add-config min.cleanable.dirty.ratio=$2" I egrep -v



SK_EXEC "SKAFKA_HOME/bin/kafka-configs.sh I
--bootstrap-server $BOOT_STRAP \
-command-config $KAFKA_HOME/config/consumer.propert


-alter
-entity-type topics
-entity-name $1 \
add-config retention. ms=$2"
::::::
. setEnv.sh


file="topics.txt"
mapfile -t lines < "Sfile"
for line in "$(lines[@])"; do
echo "topic list: $line"
$K_EXEC "$KAFKA_HOME/bin/kafka-configs.sh I
-bootstrap-server $BOOT_STRAP

-command-config SKAFKA_HOME/config/consumer.prop


-alter
-topic "$line" \
-add-config min.insync.replicas=3"
done



goSororities new sink onnectarinesIts
kafka-topics in --1100--TOPIC SHIITEPI SHIII ProcessState Transformed BOOTSTRAP-Server KALKI-70001000 9092
#Ft √ñt√ºr√ºd√º≈ü√ºnm√º≈üt√ºn * ≈ü√ºkr√ºn√º
Series- Not produces 28--topic Payload Reporting ATRACE-DAYIttap-outer kaffa-headless 5092
kafka-topics. 2--create topic Payload-Reporting-Ripeits bootstrap-server kills-headless 9092- partitions 3-- cells clearly policy deictic
Sorted tuneless as Lake tools ActorIs theIntolerance SOS? topic far Author Asp Bats
TOP-SEEDED katie headless 9092 -TOPIC PAY 0 K port no 7 11 REPEATS Off... 1--partition 0 lanne
kitts-console-consumer in --bootstrap-server kafka-headless 9092--topic Payload-Reporting-failed-RIPCATS from-beatenand
kitty console-producer at --topic & flood he often RIPCAT-bootstrap-server kafka-headless 5052
barks-run-class of kafka tools Gotoffsetshell-bentex-laxtake-atiers 9092--topic Rebuilding payment,
I of alt text gain are 093/connectors/ 1 and I post n I then It at sake connectorstatts I python In gear tool


>>>[[>[[>>>>[
metrics to view before rebalancing a partitions 
Before rebalancing partitions in Kafka, it is important to monitor several key metrics to ensure that the cluster is healthy and that the rebalancing
Wwill not cause any disruption to the ongoing data processing. Some of the key metrics to consider include:


Broker CPU and memory utilization: Rebalancing partitions may cause a temporary increase in CPU and memory utilization on the brokers.
these metrics to ensure that the brokers have sufficient resources to handle the increased load during the rebalancing process


You should monitor


increase in network traffic as the brokers exchange metadatƒÖ and data between each other. You
and the network infrastructure can handle the increased load during the rebalancing process


Network traffic: Rebalancing partitions may causeshould monitor the network traffic 
to ensure that a temporarythe brokers


Topic and partition lag: Topic and partition lag is an important metric to monitor in Kafka, as it indicates
and the time it is consumed. If there is a significant lag in a topic or partition, it may indicate that the
rate, and rebalancing partitions may not be effective in addressing the issue


the delay between the
consumer group is not


Consumer group lag: Consumer group lag is
metric that indicates the difference between the latest
processed by the consumer group. If there is a significant lag in a consumer group, it may indicate
partitions fast enough, and rebalancing partitions may not be effective in addressing the issue


offset of a partition and the offset of the last message
that the consumer group is not pulling data from the


Partition skew: Partition skew is a metric that indicates the imbalance of data distribution across partitions in a topic. If there is a significant
partition skew, it may indicate that some partitions are handling more data than others, which can lead to performance and availability issues. Rebalancing
help address partition skew and ensure a more even distribution of data across 


By monitoring these metrics before rebalancing partitions in Kafka,
cause any disruption to the ongoing data processing.


you can ensure that


the cluster is healthy and that the


rebalancing process will n

@#########
cat /var/log/kafka/server.log | grep "replication"     --->broker logs for slow fetches
kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions
bin/kafka-topics.sh --describe --topic <topic-name> --bootstrap-server <broker> | grep UnderReplicatedPartitions
bin/kafka-topics.sh --describe --topic <topic-name> --bootstrap-server <broker>
Optimize log.segment.bytes, log.retention.ms, and log.retention.bytes.



--------
kafka-configs.sh --bootstrap-server <broker-address> --entity-type brokers --entity-name <broker-id> --describe
kafka-configs.sh --bootstrap-server <broker-address> --entity-type brokers --entity-name <broker-id> --alter --add-config num.network.threads=8,num.io.threads=4"
kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name <topic-name> --alter --add-config "retention.ms=1000"
delete after 1 second

--add-config cleanup.policy=compact
--add-config  delete.retention.ms=10000
--add-config segment.ms=1000
-add-config min.cleanable.dirty.ratio=0.01
--add-config min.compact.lag.ms=500


/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name __consumer_offsets --alter --add-config "retention.ms=86400000"  # Retention for 1 day
/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name <topic-name> --alter --add-config  cleanup.policy=compact"

kafka-topics.sh --bootstrap-server localhost:9092 --create --topic employee-salary  --partitions 1 --replication-factor 1  --config cleanup.policy=compact  --config min.cleanable.dirty.ratio=0.001  --config segment.ms=5000


# cluster-wider:
$ bin/kafka-configs.sh 
  --bootstrap-server host:port 
  --entity-type brokers 
  --entity-default # cluster-wider
  --alter 
  --add-config unclean.leader.election.enable=true

# per-broker:
$ bin/kafka-configs.sh 
  --bootstrap-server host:port 
  --entity-type brokers 
  --entity-name 1 # per-broker
  --alter 
  --add-config unclean.leader.election.enable=false


bin/kafka-topics.sh --describe --topic __consumer_offsets --bootstrap-server localhost:9092 
bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 1 --alter --add-config "offsets.retention.minutes=20160"


--

message.max.bytes  Meaning: Maximum size of a message;
log.cleaner.enabled  Log compacted: For each key, only the latest message is kept
----

kafka-configs.sh --bootstrap-server <KAFKA_BOOTSTRAP_SERVER> \
  --alter --entity-type topics --entity-name $topic \
  --add-config retention.ms=-1


kafka-configs.sh --bootstrap-server <KAFKA_BOOTSTRAP_SERVER> \
  --alter --entity-type topics --entity-name $topic \
  --add-config cleanup.policy=compact,min.cleanable.dirty.ratio=0.1,segment.bytes=4194304


kafka-topics.sh --bootstrap-server <KAFKA_BOOTSTRAP_SERVER> \
  --describe --topic $topic


. Check Topic Retention Configuration
To view the current retention policy for a topic:


bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type topics --entity-name <topic-name>
Look for the retention.ms, retention.bytes, and cleanup.policy properties.

2. Set or Update Data Retention Policy
Modify the retention configuration for an existing topic:

a) Retention Based on Time (retention.ms)
Example: Keep data for 7 days (7 days * 24 hours * 60 minutes * 60 seconds * 1000 ms):

bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name <topic-name> --alter --add-config "retention.ms=604800000"

b) Retention Based on Size (retention.bytes)
Example: Limit the topic size to 1 GB:


bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name <topic-name> 
--alter --add-config "retention.bytes=1073741824"

c) Set Cleanup Policy (cleanup.policy)
Options: delete (default) or compact
Example to enable log compaction:

bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type topics --entity-name <topic-name> --alter --add-config "cleanup.policy=compact"

3. Verify Updated Retention Settings
To confirm the changes:

bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --entity-type topics --entity-name <topic-name>
4. Broker-Level Retention Configurations (Global Settings)
You can set retention configurations globally in server.properties:

log.retention.ms=604800000      # Retention period (7 days)
log.retention.bytes=1073741824  # 1 GB per partition
log.segment.bytes=1073741824    # Segment file size
log.cleanup.policy=delete       # Cleanup policy
Restart the Kafka broker after making changes:

bin/kafka-server-start.sh config/server.properties


5. Remove Data from a Topic Manually
To delete all data from a topic:


bin/kafka-topics.sh --bootstrap-server localhost:9092 \
--delete --topic <topic-name>



----

Putty | mobaxterm >> Download

Kubernetes (K8S)
 
 sudo -i
 hostnamectl set-hostname master // same way set hostname as worker1 and worker2 for other 2 machines
 bash
 
 nano script.sh     // ctrl+o to save (ENTER) and ctrl+x  (ENTER) to exit
 
 ### Installation Script  ################
 create scriptfile vi script.sh
 
 
#!/bin/bash 
sudo apt update -y
sudo apt install apt-transport-https ca-certificates curl software-properties-common  -y
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable"
sudo apt update -y
apt-cache policy docker-ce -y
sudo apt install docker-ce -y
wget -q -O - https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
echo deb http://apt.kubernetes.io/ kubernetes-xenial main | sudo tee /etc/apt/sources.list.d/kubernetes.list
apt update -y
apt install kubelet=1.21.1-00 kubeadm=1.21.1-00 kubectl=1.21.1-00 -y
sysctl net.bridge.bridge-nf-call-iptables=1

################
 
sh script.sh  // to run script -- RUN on all machine

#RUN only on master

kubeadm init --pod-network-cidr=192.168.0.0/16 >> cluster_initialized.txt

tail -f cluster_initialized.txt
---------------

on master node from the above file and also shows the join command.

mkdir -p $HOME/.kube >> Create both .kube and its parent directory
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  >> copy configuration to kube directoy
  sudo chown $(id -u):$(id -g) $HOME/.kube/config >> provide user configuration to config file

kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml >> network

curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O

or 

kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml


====================
If not working then we have to reset kubeadm >>

kubeadm reset
rm -rf $HOME/.kube/config

=====================

  mkdir -p $HOME/.kube >> Create both .kube and its parent directory
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  >> copy configuration to kube directoy
  sudo chown $(id -u):$(id -g) $HOME/.kube/config >> provide user configuration to config file

  kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
  systemctl restart kubelet.service
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Paste join scripts on worker nodes

========================================================================================================================

if getting error >> The connection to the server localhost:8080 was refused 

either not using the correct user or follow below step :

kubeadm reset
rm -rf $HOME/.kube/config

=====================

  mkdir -p $HOME/.kube >> Create both .kube and its parent directory
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  >> copy configuration to kube directoy
  sudo chown $(id -u):$(id -g) $HOME/.kube/config >> provide user configuration to config file
=========================================================================================================================

important Files :

/etc/kubernetes/pki >> certificates
/etc/kubernetes/manifests >> static pods
/etc/kubernetes/ >> below config files where pki has all the certificates
admin.conf  controller-manager.conf  kubelet.conf  manifests  pki  scheduler.conf

===================================================================================================
kubectl get pods -A -o wide
NAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE   IP                NODE              NOMINATED NODE   READINESS GATES
default       pardeep                                    1/1     Running   0          13m   192.168.84.2      sandeep-worker2   <none>           <none>
default       sandeep                                    1/1     Running   0          16m   192.168.184.129   sandeep-worker1   <none>           <none>
default       sumit                                      1/1     Running   0          13m   192.168.84.1      sandeep-worker2   <none>           <none>
kube-system   calico-kube-controllers-69f595f8f8-zzb52   1/1     Running   0          44m   192.168.141.129   sandeep-master    <none>           <none>
kube-system   calico-node-d6wp4                          1/1     Running   0          26m   172.31.21.253     sandeep-worker1   <none>           <none>
kube-system   calico-node-hdv8p                          1/1     Running   0          44m   172.31.28.185     sandeep-master    <none>           <none>
kube-system   calico-node-xhnq9                          1/1     Running   0          27m   172.31.20.178     sandeep-worker2   <none>           <none>
kube-system   coredns-558bd4d5db-67mz6                   1/1     Running   0          46m   192.168.141.130   sandeep-master    <none>           <none>
kube-system   coredns-558bd4d5db-z6j2x                   1/1     Running   0          46m   192.168.141.131   sandeep-master    <none>           <none>
kube-system   etcd-sandeep-master                        1/1     Running   0          47m   172.31.28.185     sandeep-master    <none>           <none>
kube-system   kube-apiserver-sandeep-master              1/1     Running   0          47m   172.31.28.185     sandeep-master    <none>           <none>
kube-system   kube-controller-manager-sandeep-master     1/1     Running   0          47m   172.31.28.185     sandeep-master    <none>           <none>
kube-system   kube-proxy-d5nh5                           1/1     Running   0          46m   172.31.28.185     sandeep-master    <none>           <none>
kube-system   kube-proxy-l9znx                           1/1     Running   0          26m   172.31.21.253     sandeep-worker1   <none>           <none>
kube-system   kube-proxy-qd5hk                           1/1     Running   0          27m   172.31.20.178     sandeep-worker2   <none>           <none>
kube-system   kube-scheduler-sandeep-master              1/1     Running   0          47m   172.31.28.185     sandeep-master    <none>           <none>


==================================================================================================


then 
kubectl get nodes
kubectl label node sandeep-worker1 node-role.kubernetes.io/worker=worker   >> to set the role to the worker node
kubectl get pods
kubectl get pods -A
kubectl get pods -A | grep -i worker1

### Run on worker1 ########
docker ps and docker rm to remove a container...

#################################
cat /root/.kube/config

======================================

kubectl get pods -A
ls -l /root/.kube/config
docker ps
ps -ef
docker ps   >> will show all the pods including the manager pod
cd /
clear
kubectl get pods
kubectl delete pod pod1
clear
kubectl run gagan-app --image httpd
kubectl get pods
kubectl get pods -o wide
curl 192.168.235.130
kubectl describe pod gagan-app | more
kubectl get pods
kubectl logs gagan-app
kubectl exec -it gagan-app -- /bin/bash
        to install curl and ping in the container for testing
        apt update -y
        apt-get install curl iputils-ping -y
==================================================

kubectl create deployment myapp1 --image httpd --replicas=2
kubectl get pods
kubectl get pods -o wide
kubectl get deployments
kubectl describe deploy myapp1
kubectl get pods
kubectl delete pod myapp1-7d8bc764bd-nhkwr
kubectl get pods
kubectl scale deployment myapp1 --replicas=4
kubectl get pods
kubectl get pods -o wide
kubectl scale deployment myapp1 --replicas=1
kubectl get pods -o wide
kubectl delete pod myapp1-7d8bc764bd-szbsm
kubectl get pods -o wide
kubectl delete deploy myapp1
kubectl get pods -o wide

==================================================

kubectl create deployment myapp1 --image httpd:2.4 --replicas=4
kubectl get deployment
kubectl describe deploy myapp1
kubectl get rs
kubectl get replicaset
kubectl describe rs myapp1-744c9889bd
kubectl get pods
kubectl delete pod myapp1-744c9889bd-tn8ll
kubectl get pods
kubectl describe rs myapp1-744c9889bd
kubectl get pods
kubectl describe pod myapp1-744c9889bd-fprnq
kubectl get all
kubectl delete rs myapp1-744c9889bd
kubectl get all
kubectl delete deployment myapp1
kubectl get all

===================================================
telecom-app:v1, v2, v3

deployment telecom-app
rs -- v1, v2, v3
pods - pod1:v2   pod2:v2   pod3:v2


vi pod.yml

apiVersion: v1
Kind: Pod
Metadata:
  Name: sandy-pod
Spec:
  Containers:
    - name: container1
      Image: httpd









 1¬† clear
¬†¬†¬† 2¬† vi script.sh
¬†¬†¬† 3¬† sh script.sh
¬†¬†¬† 4¬† kubeadm init --pod-network-cidr=10.10.0.0/16 >> cluster_initialized.txt
¬†¬†¬† 5¬† ls
¬†¬†¬† 6¬† cat cluster_initialized.txt
¬†¬†¬† 7¬† mkdir -p $HOME/.kube
¬†¬†¬† 8¬† cat $HOME/.kube/config
¬†¬†¬† 9¬† kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml
¬†¬† 10¬† kubectl get nodes
¬†¬† 11¬† kubectl get nodes -w
¬†¬† 12¬† kubectl get nodes
¬†¬† 13¬† kubectl get nodes -w
¬†¬† 14¬† kubectl get nodes
¬†¬† 15¬† kubectl get pods
¬†¬† 16¬† kubectl get ns
¬†¬† 17¬† kubectl get pods -n kube-system
¬†¬† 18¬† kubectl get pods -n kube-system -o wide
¬†¬† 19¬† kubectl run pod1 --image nginx
¬†¬† 20¬† kubectl get pods
¬†¬† 21¬† kubectl describe pod pod1
¬†¬† 22¬† kubectl get pods -o wide
¬†¬† 23¬† docker ps
¬†¬† 24¬† clear
¬†¬† 25¬† kubectl get pods
¬†¬† 26¬† kubectl get pods -o wide
¬†¬† 27¬† kubectl get pods -n kube-system
¬†¬† 28¬† kubectl get pods -all
¬†¬† 29¬† kubectl get pods --all
¬†¬† 30¬† kubectl get pods -a
¬†¬† 31¬† kubectl get pods -A
¬†¬† 32¬† kubectl get pods -A -o wide
¬†¬† 33¬† docker ps
¬†¬† 34¬† clear
¬†¬† 35¬† cd /etc/kubernetes/
¬†¬† 36¬† ls
¬†¬† 37¬† cd manifests/
¬†¬† 38¬† ls
¬†¬† 39¬† kubectl get pods
¬†¬† 40¬† kubectl get pods -o wide
¬†¬† 41¬† kubectl get pods -A
¬†¬† 42¬† kubectl create deploy frontend --image nginx --replicas=4 --dry-run=client -o yaml > deploy.yaml
¬†¬† 43¬† vi deploy.yaml
¬†¬† 44¬† kubectl apply -f deploy.yaml
¬†¬† 45¬† kubectl get pods
¬†¬† 46¬† kubectl get pods -o wide
¬†¬† 47¬† kubectl get svc
¬†¬† 48¬† history

journalctl -u kafka.service | grep inter.broker.protocol.version
journalctl -u kafka.service | grep log.message.format.version
kafka-topics --bootstrap-server worker1.bigdata.com:9092,worker2.bigdata.com:9092,worker3.bigdata.com:9092 --list
kafka-console-consumer --bootstrap-server worker1.bigdata.com:9092,worker2.bigdata.com:9092,worker3.bigdata.com:9092 --topic test --from-beginning
Restart the brokers from Admin Machine using Roll Restart Script.
kafka-rolling-restart --cluster-type kafka --check-count 7

command
|bin/kafka-producer-perf-test.sh --topic perf-test1 -num-records 1 --record- size 1000 --throughput 2000 --producer-props bootstrap. servers-105:9092 :
/bin/kafka-producer-perf-test.sh -topic -test1 --num-records 10 --record-size 1080 --throughput 2000 --producer-props bootstrap. servers-10.5:9092 
/bin/kafka-producer-perf-test.sh --topic perf-test1 --num-records 100 --record-size 1080 --throughput 2030 --producer-props bootstrap. servers=10.9092 
./bin/kafka-producer-perf-test.sh --topic perf-testi --nun-records 300 --record-size 1000 --throughput 2000 --producer-props bootstrap. servers-10.5:9092 
:/bin/kafka-producer-perf-test.sh --topic perf-test1 --num-records 5e0 --record-size 1000 --throughput 2000 --producer-props bootstrap. servers-10.5:9392 
:/bin/kafka-producer-perf-test.sh --topic perf-testi -- num-records 700 --record-size 1000 --throughput 2023 -- producer-props bootstrap. servers=10.9092 
:/bin/kafka-producer-per-f-test.sh --topic perf-test1 --num-records 1000 --record-size 1000 --throughput 2003 -- producer-props bootstrap. servers-105:9092 
./bin/kafka-producer-perf-test.sh --topic perf-test1 --numn-records 10800 --record-size 1000 --throughput 2080 --producer-props bootstrap. servers=10:9092 
./bin/kafka-producer-perf-test.sh --topic perf-testl --num-records 10e00 --record-size 5000 --throughput 2303 --produser-props bootstrap. servers=10:9092 
./bin/kafka-producer-perf-test.sh --topic perf-testl --num-records 10000 --record-size 100880 --throughput 2033 --produ| Hint: double-click to


-------

Updating Configurations using kafka-configs command.

	# Per Broker Config
	kafka-configs --bootstrap-server bigdata.cluster.broker1:9092,bigdata.cluster.broker2:9092,bigdata.cluster.broker3:9092 --entity-type brokers --entity-name 132 --alter --add-config log.cleaner.threads=2

	# Describe Configs Per Broker
	kafka-configs --bootstrap-server bigdata.cluster.broker1:9092,bigdata.cluster.broker2:9092,bigdata.cluster.broker3:9092 --entity-type brokers --entity-name 132 --describe

	# Delete Config Per Broker
	kafka-configs --bootstrap-server bigdata.cluster.broker1:9092,bigdata.cluster.broker2:9092,bigdata.cluster.broker3:9092 --entity-type brokers --entity-name 132 --alter --delete-config log.cleaner.threads

	# Cluster-wide Config
	kafka-configs --bootstrap-server bigdata.cluster.broker1:9092,bigdata.cluster.broker2:9092,bigdata.cluster.broker3:9092 --entity-type brokers --entity-default --alter --add-config log.cleaner.threads=2


--------
Produce and Consume Binary Messages

Producing Binary Messages:

echo "First message with base64 encoders" | base64

Rmlyc3QgbWVzc2FnZSB3aXRoIGJhc2U2NCBlbmNvZGVycwo=

curl -X POST -H "Content-Type: application/vnd.kafka.binary.v2+json" \
      -H "Accept: application/vnd.kafka.v2+json" \
      --data '{"records":[{"value":"Rmlyc3QgbWVzc2FnZSB3aXRoIGJhc2U2NCBlbmNvZGVycwo="}]}' "http://192.168.0.112:8082/topics/test"

Output:

{
    "offsets":
    [
        {
            "partition":0,
            "offset":3,
            "error_code":null,
            "error":null
        }
    ],
    "key_schema_id":null,
    "value_schema_id":null
}

Consuming Binary Messages:

Creating the consumer:

curl -X POST -H "Content-Type: application/vnd.kafka.v2+json" \
      --data '{"name": "rest_consumer_1", "format": "binary", "auto.offset.reset": "earliest"}' \
      http://192.168.0.112:8082/consumers/rest_consumer_1

Output:

{
    "instance_id":"rest_consumer_1",
    "base_uri":"http://192.168.0.112:8082/consumers/rest_consumer_1/instances/rest_consumer_1"
}

Subscribe to the topic:

curl -X POST -H "Content-Type: application/vnd.kafka.v2+json" --data '{"topics":["test"]}' \
      http://192.168.0.112:8082/consumers/rest_consumer_1/instances/rest_consumer_1/subscription


Read the records from the topic:

curl -X GET -H "Accept: application/vnd.kafka.binary.v2+json" \
      http://192.168.0.112:8082/consumers/rest_consumer_1/instances/rest_consumer_1/records

Output :

[
    {
        "topic":"test",
        "key":null,
        "value":"Rmlyc3QgbWVzc2FnZSBmcm9tIEphdmEgY29kZSBmb3IgYSBLZXJiZXJvcyBBdXRoZW50aWNhdGVkIFRvcGljIQ==",
        "partition":1,
        "offset":2
    },
    {
        "topic":"test",
        "key":null,
        "value":"Rmlyc3QgbWVzc2FnZSB3aXRoIGJhc2U2NCBlbmNvZGVycwo=",
        "partition":0,
        "offset":3
    }
]




----------1. Validate Kafka Broker Connectivity
The first step is ensuring that your application can connect to the Kafka brokers.

Steps:
Use Telnet/Netcat (Basic Check):

bash
Copy code
telnet <broker-host> <broker-port>
or

bash
Copy code
nc -zv <broker-host> <broker-port>
Replace <broker-host> and <broker-port> with your broker's address and port (default: 9092).

Expected Result:

If connectivity is fine, you’ll get a response like Connected.
If there’s an issue, ensure the Kafka brokers are reachable from the application’s network.
2. Test Kafka Cluster Access with CLI
Steps:
Use the kafka-topics.sh script to list topics from the Kafka cluster:
bash
Copy code
kafka-topics.sh --bootstrap-server <broker-host>:<broker-port> --list
If you can list the topics, it confirms basic connectivity and authentication.
3. Verify Producer and Consumer Functionality
Set up a test producer and consumer to validate end-to-end functionality.

Test Producer:
Run a producer to send a test message:

bash
Copy code
kafka-console-producer.sh --broker-list <broker-host>:<broker-port> --topic <test-topic>
Type a message (e.g., Hello Kafka) and press Enter.

Test Consumer:
Run a consumer to check if the message is received:

bash
Copy code
kafka-console-consumer.sh --bootstrap-server <broker-host>:<broker-port> --topic <test-topic> --from-beginning
If the consumer receives the message, the connectivity is fine.
4. Application-Specific Connectivity Test
Check if the external application can interact with Kafka by producing or consuming messages programmatically.

Example in Java:
java
Copy code
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import java.util.Properties;

public class KafkaConnectivityTest {
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put("bootstrap.servers", "<broker-host>:<broker-port>");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        KafkaProducer<String, String> producer = new KafkaProducer<>(props);
        producer.send(new ProducerRecord<>("test-topic", "key", "Hello Kafka!"));
        producer.close();
    }
}
Run this application. If it works without errors, the connectivity is fine.
5. Validate Using Monitoring Tools
Use Kafka monitoring tools to check if the application is connecting to the Kafka cluster:

Kafka Manager: Check active brokers and consumer groups.
Confluent Control Center: Monitor producers, consumers, and message throughput.
Metrics with JMX: Ensure the application is visible in Kafka metrics.
6. Debugging Connectivity Issues
Check Firewalls:
Ensure the Kafka broker ports are open and reachable.
Broker Advertised Listeners:
Verify that Kafka is configured with appropriate advertised.listeners for external access.
Authentication:
If SASL/SSL is enabled, ensure the application has the correct credentials and certificates.
7. Automating Connectivity Tests
Use tools like kcat (formerly kafkacat) to script connectivity checks:

bash
Copy code
kcat -b <broker-host>:<broker-port> -L
Lists metadata about brokers and topics.
8. Check Logs
Check application and Kafka logs for errors related to connectivity, timeouts, or misconfigurations.
By systematically following these steps, you can confirm that the external application can successfully connect and interact with the Kafka cluster.







helm commands :

helm list myapp sta/chart2 --version  "0.1.2"
  helm status myapp --show-resources
  helm list --superseded
  helm get values myapp --revision ` 2
  helm history  myapp
  helm install myapp char1 --set service.nodePort=3210
  helm get manifest myapp --revision
  helm get manifest  releasename  --revision
  helm get all myapp
  helm get value
  hlm get hooks releasename 


kubectl auth can-i create deployments --namespace=small --as=$READER_USER_ID

kubectl auth can-i create deployments 

$ kubectl auth can-i create deployments
$ kubectl auth can-i create deployments --as bob
$ kubectl auth can-i create deployments --as bob --namespace developer


kubectl get pods -o wide --show-labels'
kubectl get services -o wide --show-labels'
kubectl get deployments -o wide --show-labels'
kubectl auth can-i --list

  kubectl auth
  192  kubectl auth can-i create pod
  193  kubectl auth can-i delete ns
  194  kubectl auth can-i -h
  
  kubectl create role developer --resource=pods --verb=create,list,get,update,delete --namespace=development
  
  kubectl --token=$JWT auth can-i --list
  
  helm list myapp sta/chart2 --version  "0.1.2"
  helm status myapp --show-resources
  helm list --superseded
  helm get values myapp --revision ` 2
  helm history  myapp
  helm install myapp char1 --set service.nodePort=3210
  helm get manifest myapp --revision
  helm get manifest  releasename  --revision
  helm get all myapp
  helm get value
  hlm get hooks releasename 
  
  
You can filter by Helm labels:
kubectl get deployments -n <namespace> -l "app.kubernetes.io/instance=<release-name>"

Or list all resources (including deployments) created by a Helm release:
helm get manifest <release-name> -n <namespace> | grep -A5 'kind: Deployment'


To see all associated resources:
kubectl get all -n <namespace> -l "app.kubernetes.io/instance=<release-name>"


✅ Get Values Used:
helm get values <release-name> -n <namespace>

✅ Get All Manifest:
helm get manifest <release-name> -n <namespace>

✅ Get Notes (post-install info):
helm get notes <release-name> -n <namespace>

✅ Get Chart Metadata:
helm get all <release-name> -n <namespace>


#kubectl get deploy
#kubectl describe deploy mydeployments
#kubectl get rs
#kubectl get pods
#kubectl scale --replicas=1 deploy mydeployments  (pods reduce 2 to 1)

#kubectl rollout status kind(deployment) objectname(mydeployments)

#kubectl rollout history kind(deployment) objectname(mydeployments)
#kubectl rollout undo deploy/mydeployments


# Aliases work with completion
alias k="kubectl"
alias kk="kubectl"
alias kg="kubectl get"
alias kd="kubectl describe"
alias krm="kubectl delete"
alias krm-9="kubectl delete --force --grace-period=0"
alias ktail="kubectl logs --tail=10 --follow"
alias ktail0="kubectl logs --tail=0 --follow"
alias kscale0="kubectl scale --replicas=0"
alias kscale1="kubectl scale --replicas=1"
alias kscale3="kubectl scale --replicas=3"


# Prepare system for alias 

alias k='kubectl'
alias kgn='kubectl get nodes -o wide'
alias kgp='kubectl get pods -o wide'
alias kgpl='kubectl get pods -o wide --show-labels'
alias kgsl='kubectl get services -o wide --show-labels'
alias kgdl='kubectl get deployments -o wide --show-labels'



# Set up fresh storage
kubectl apply -f jupyterhub/storage/kadalu/kadalu-replicated-storage.yaml
kubectl apply -f jupyterhub/storage/kadalu/kadalu-pvc.yaml

# wait for it to finish setting up

# patch pv to be persistent
kubectl patch pv $(kubectl get pv -o=jsonpath='{.items[?(@.spec.claimRef.name=="kadalu-replicated-pvc")].metadata.name}') -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'

# Backup k8s kadalu config
kubectl get KadaluStorage replicated-storage -n default -o yaml > kadalu-storage-backup.yaml
kubectl get pv $(kubectl get pv -o=jsonpath='{.items[?(@.spec.claimRef.name=="kadalu-replicated-pvc")].metadata.name}') -o yaml > pv-backup.yaml
kubectl get pvc kadalu-replicated-pvc -n jupyterhub -o yaml > pvc-backup.yaml


# backup the glusterfs files
kubectl exec -n kadalu server-replicated-storage-0-0 -- bash -c "tar -czvf /tmp/glusterfs-metadata-backup.tar.gz /bricks/replicated-storage/data/brick/.glusterfs"
kubectl cp kadalu/server-replicated-storage-0-0:/tmp/glusterfs-metadata-backup.tar.gz ./server-0-glusterfs-backup.tar.gz -n kadalu
kubectl exec -n kadalu server-replicated-storage-1-0 -- bash -c "tar -czvf /tmp/glusterfs-metadata-backup.tar.gz /bricks/replicated-storage/data/brick/.glusterfs"
kubectl cp kadalu/server-replicated-storage-1-0:/tmp/glusterfs-metadata-backup.tar.gz ./server-1-glusterfs-backup.tar.gz -n kadalu
ls -lh ./server-*-glusterfs-backup.tar.gz

# Do all the things to reset the cluster

# appy th replicated storage backup and immediatly pause kadalu
kubectl apply -f kadalu-storage-backup.yaml
kubectl scale deployment operator --replicas=0 -n kadalu
kubectl get deployments -n kadalu

# Copy the glusterfs files back
kubectl exec -n kadalu server-replicated-storage-0-0 -- rm -rf /bricks/replicated-storage/data/brick/.glusterfs
kubectl cp ./server-0-glusterfs-backup.tar.gz kadalu/server-replicated-storage-0-0:/tmp
kubectl exec -n kadalu server-replicated-storage-0-0 -- tar -xzvf /tmp/server-0-glusterfs-backup.tar.gz -C /bricks/replicated-storage/data/brick/
kubectl exec -n kadalu server-replicated-storage-1-0 -- rm -rf /bricks/replicated-storage/data/brick/.glusterfs
kubectl cp ./server-1-glusterfs-backup.tar.gz kadalu/server-replicated-storage-1-0:/tmp
kubectl exec -n kadalu server-replicated-storage-1-0 -- tar -xzvf /tmp/server-1-glusterfs-backup.tar.gz -C /bricks/replicated-storage/data/brick/


kubectl scale deployment operator --replicas=1 -n kadalu
----------

Task
Write a Helm template to:

Generate volumeMounts from a list in values.yaml.
Each mount should include additional metadata using dict.
values.yaml
yaml
Copy code
volumes:
  - name: config-volume
    path: /etc/config
  - name: data-volume
    path: /var/data
template.yaml
yaml
Copy code
volumeMounts:
{{- range .Values.volumes }}
  {{- $mount := dict "name" .name "path" .path }}
  - name: {{ $mount.name }}
    mountPath: {{ $mount.path }}
{{- end }}
Expected Output
yaml
Copy code
volumeMounts:
  - name: config-volume
    mountPath: /etc/config
  - name: data-volume
    mountPath: /var/data
---

env:
{{- range $index, $item := .Values.config }}
  - name: {{ $item.key | upper }}
    value: "{{ $item.value }}"
{{- end }}

---

Exercise 3: Passing Context to an Included Template
Task
Create an included template that generates an environment variable block for a container. Pass data as context to customize the output.

----

Exercise 2: Nesting Includes
Task
Use a nested include to generate resource names dynamically based on a prefix and suffix.

Steps
Define a helper template in _helpers.tpl for the prefix:

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "mychart.fullResourceName" (dict "name" "deployment" "context" .) }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: {{ include "mychart.fullResourceName" (dict "name" "app" "context" .) }}
  template:
    metadata:
      labels:
        app: {{ include "mychart.fullResourceName" (dict "name" "app" "context" .) }}
    spec:
      containers:
      - name: {{ include "mychart.fullResourceName" (dict "name" "container" "context" .) }}
        image: nginx

----


kind: Deployment


 minikube version
   14  echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
   16  sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
   17  kubectl version
   18  kubectl version --client
   19  minikube start --driver=docker --force
   20  minikube status---
docker logs kafka
docker logs zookeeper
``>

Look for confirmation messages:
- Kafka: **"Started NetworkTrafficServerConnector"** or **"Kafka server started"**.
- ZooKeeper: **"binding to port 0.0.0.0/0.0.0.0:2181"**.

---

### **2. Verify Service Resolution**
Inside the Docker network, check if Kafka and ZooKeeper can resolve each other's names using DNS:
```bash
docker exec kafka ping zookeeper
docker exec zookeeper ping kafka




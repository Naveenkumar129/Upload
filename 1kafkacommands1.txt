import groovy.transform.Field
eField List<String> topiclistCached = []
 
static def _kubectl(String kubect1, String config, String namespace, String cmd) ( return "Sfkubectl) --kubeconfig=$(config) -n $(namespace) exec kafka-broker4-0 -c kafka-broker "bash -C 'KAFKA_LOG4)_OPTS=-Dlog4j.configuration=file://kafka_2.12-3.3.1/config/tools-log4j.properties " "kafka-configs.sh --bootstrap-server 128.168.112.26:9093,128.168.112.27:9093,128.168.112.28:9093,128.179.18.26:9093,128.179.18 "Sfcmd) " I

static def kafkaTopic(String kubectl, String config, String namespace, String cmd) ( return "S(kubectl) --kubeconfig-sfconfig) -n $(namespace) exec kafka-broker4-0 -c kafka-broker "bash -c 'KAFKA_LOG4)_OPTS--Dlog4j.configuration=file:///kafka_2.12-3.3.1/config/tools-log4j.properties "+ "kafka-topics.sh --bootstrap-server kafka-headless:9092 --command-config /kafka_2.12-3.3.1/config/producer.properties " "S(cmd)" 2>/dev/null"

 def topicExist (String config, String namespace, String kubectl, String topic) ( if (!topicListCached) ( topiclistCached = sh(script: kafkaTopic(kubectl, config, namespace, '--list'), returnStdout: true) .trim().split("in')

def addTopic(String config, String namespace, String kubect1, String topic, String partitions, String policy) f topiclistCached = [] "--create --topic $(topic) --partitions $(partitions) --config cleanup.policy=$fpolicy)" def cmd return sh(script: kafkaTopic(kubectl, config, namespace, cmd), returnStatus: true)

def setPartition(String config, String namespace, String kubectl, String topic, String partitions) ( def cmd "--alter --topic $ftopic) --partitions $(partitions)" return sh(script: kafkaTopic(kubectl, config, namespace, cmd), returnStatus: true)


 def setCleanUp(String config, String namespace, String kubectl, String topic, String policy) ( def cmd = "--alter --entity-type topics --entity-name Sftopic) --add-config cleanup.policy-S(policy)" return sh(script: kafkaTopic(kubectl, config, namespace, cmd), returnStatus: true)


 def handleTopicConfigs(String config, String envToProvision, String kubectl, String topicName, String partitionsNumber, String cleanup printin "Handling topic S(topicNamet, configuring SfpartitionsNumber) partitions and a S(cleanupPolicy) cleanup policy. if (!topicExist(config, envToProvision, kubectl, topicName)) ( addTopic(config, envToProvision, kubectl, topicName, partitionsNumber, cleanupPolicy) )else setPartition(config, envToProvision, kubectl, topicName, partitionsNumber) setCleanUp(config, envToProvision, kubectl, topicName, cleanupPolicy)

----
#Consume messages kafka-console-consumer.sh To get Offline partition
 kafka-topics.sh bootstrap-server 0.0.0.0:9093 command-config /kafka_2.12-3.3.1/config/producer.properties --describe I grep "Leader: None" I kafka-consumer-groups.sh -bootstrap-server kafka-headless:9092 --group connect-jdbc-new2-sink- connector --reset-offsets --to-earliest --topic test
 -from-beginning
kafka-consumer-groups.sh
-bootstrap-server kafka-headless:9092
execute
group connect-jdbc-new2-sink-connector
-reset-offsets --shift-by 100
topic test
-execute
------
 Ielnęt Broker to check connectivity Place IP's in ip.txt file and run below command
while true; do for i in $(cat iR.tXt) do timeout 3s curl
 -v telnet://$i ;
echo
**kk ";
done
done
----------------
ACTIVE CONTROLLER UNDER REPLICATED PARTITIONS
katką-topics.sh --under-replicated-partitions --describe --bootstrap-server 
TO GET UNDER REPLICATED PARTITIONS COUNT Alter Topic :
ķafka-topics.sh --bootstrap-server <List of IPs> --alter /kafka_2.12-3.3.1/confiq/consumer.properties
 0.0.0.0:9093
--command-config /kafka_2.12-3.3.1/config/producer.properties
--topic <Topic-name> --partitions
<no. of partition>
--command-contig
Alter cleanup Policy : kafka-configs.sh --bootstrap-server <List of IPs> --alter --entity-type topics --entity-name GleanuR.pŢicy=' <delete/ compact> -command-coafiq /kafka_ 2.12-3.3.1/ contig/ consumer.properties
 <Topic-name>
--add-config
Retention Time : kafką-confiqs.sh --bootstrap-server <List of IPs> --alter --entity-type topics --entity-name retention.ms=cretention-time in.milisscond> --command-config /kafka_2.12-3.3.1/config/consumer.properties
 <Topic-name>
--add-config
ķafka-confiqs.sh --bootstrap-server <Iist --add-config retention.bytes=21474836480 or katka-topics.sh --zookeerer <List of IPs> =-alter
 of IPs> /kafka 2.12-3.3.1/config/producer.properties
--alter
--entity-type topics
--entity-name
Correlation-Store
--topic Correlation-Store --config retention.bytes=21474836480
/kafka_2.12-3.3.1/confiq/producer.proper

_____<

FileEdit ormat iew Help Get all consumer groups names: kafka-consumer-groups.sh --all-groups --list
 -bootstrap-server localhost:9092
Find consumer groups using our topic: for group in $(cat groups.txt); do kafka-consumer-groups. .sh --bootstrap-server localhost:9092 --group $group --describe grep -i pol-sms-pdp retention-updates; if ["$?" == 0 ]; then echo $group >> groups₩ithRequiredTopic.txt ; fi; done
 Once done, you can get required consumer-group name: pol-sms-message-entrypoint
This command may take some time to execute.
Then, you can reset offset to latest using below:
kafka-consumer-groups.sh
-bootstrap-server localhost:9092
group pol-sms-message-entrypoint --reset-offsets --to-latest --topic pol-sms-pdp-retention-updates
-topic <topicName>
-bootstrap-server localhost:9092
#Produce messages kafka-console-producer.sh
 bootstrap-server localhost:9092 --topic <topicName>

------------

kafka-get-offsets --bootstrap-server cit-c.:9092 --topic Throttling-Circuit-Breaker --time -2 command-config /mnt/confiq/client.properties Latest offsets kafka-get-offaeta.sh --bootstrap-server 0.0.0.0:9093 --topic Throttling-Circuit-Breaker --time -l --command-config /kafka_2.12-3.3.1/config/producer.prope. Earliest Offset kafka-get-offaets.sh --bootstrap-server 0.0.0.0:9093 --topic Throttling-Circuit-Breaker time -2 --command-config /kafka_2.12-3.3.1/config/producer.proper. kafka-dump-log.sh --print-data-log --files /tmp/kafka-logs/AMH-EU-Urgent-Throttler-0/00000000000000000010.log
=---------

Search message by value format Collapse source
/opt/connect/kafka/software/confluent/bin/kafka-console-consumer I --bootstrap-server $(hostl:postl),$(host2:post2) \ --consumer.config /opt/connect/kafka/software/confluent/ctc/kafka/client.properties I -partition $(partitionNum) --offset $(offsetNumber) \ -topic $(topicName) \ -property print.key-true I --property print.timestamp-true I --property print.headers=true I -timeout-ms 2000 \ -max-messages 30 \ laxk -F 'CreateTime:l\t' '$2>-$(startTimestamp) &s $2<=$(endTimestamp) (print $0)' > /tmp/kafka/tmp-mesConsole-1c

Search message by value format Collapse source
/opt/connect/kafka/software/confluent/bin/kafka-console-consumer --bootstrap-server $(hostl:postl),$(host2:post2) \ --consumer.config /opt/connect/kafka/software/confluent/etc/kafka/client.properties -partition $(partitionNum) -offset $(offsetNumber) \ -topic $(topicName) \ -property print.key-true I -property print.timestamR-true --property print.headers=true -timsout-ms 2000 \ -nax-messages 30 lawk -F 'CreateTime: It' "$2>=1701919762020 (print $o' /tmR/ kafka EIR -mesConsole-log/1.txt 

Get topic partition offset by timestamp format Collapse source
/opt/connect/kafka/software/confluent/bin/kafka-get-offsets --bootstrap-server $(hostl:postl),$(host2:post2) \ -command-config /opt/connect/kafka/software/confluent/etc/kafka/client.properties --topic-partitions $(topicName):$(partitionNum) --time $(timestamr)

 Get messages by offset and topic partition format Collapse source T /opt/connect/kafka/software/confluent/bin/kafka-console-consumer I -bootstrap-server $(hostl:postl),$(host2:post2) \ -consumer.config /opt/connect/kafka/software/confluent/etg/kafka/client.properties I -partition $(partitionNum) --offset $(offsetNumber) \ -topic $(topicName) \ -property print.key-true ( --property print.timestamp=true --property print.headers=true I -timeout-ms 2000 \ --Wax-messages 30

 Search message by value format Collapse source
/opt/connect/kafka/software/confluent/bin/kafka-console-consumer I --bootstrap-server $(hostl:postl),$(host2:post2] \ -consumer.config /opt/connect/katkg/software/confluent/stc/kafka/client.properties I -partition $(partitionNum) -offset $(offsetNumber) \ --topic $(topicNamne] \ -property print.key-true --property print.timnestamp=true -property print.headers=true -timsout-ms 2000 \ max-messages 30 \ larep $(Value) >/tmp/kafka/tmo-mesConsole-loa/$(loa file name) al text file length:2,

-----We can trigger compaction only on the topics with cleanup.policy=compact, The compaction is a regular operation on kafka and it runs many times every day for different topics.
Action
1.Get the value of the min.cleanable.dirty.ratio.
kafka-configs.sh
-bootstrap-server localhost:9093
command-config /kafka_2.12-3.3.1/config/producer.properties --entity-type topics--entity-name <TOPIC_NAME>
describe --all| get min.ceanable.dirty.ratio
bash-4.23 kafka-configs.sh --bootstrap-server localhost:9893 --cornand-config /kafka_2.12-3.3.1/config/producer.propertics-cntity-ty topics .-entity-nane Correlation-Store --describe --all I grep nin.cleanable.dirty.ratio nin.clcanablc.dirty.ratio-0.5 scnsitivc-falsc synonyns=(DYMAHIC_TOPICfONFIG:nin.cleanablc.cirty.ratio-0.5, DEFAULT_COFIG:log.clear nin.cleanable.ratio=0.5)

 2. Reduce the value of min.cleanable.dirty.ratio ie 0.1
kafka-configs.sh --bootstrap-server localhost:9093 --command-config/kafka_2.12-3.3.1/config/producer.properties entity-type topics --entity-name <TOPIC_NAME> --alter --add-config min.cleanable.dirty.ratio=<VAL
[2074-01-10 12:23:31,333) INFO Kofko cormitId: e23c59d08c687ff5 (org.apache.kafka.cornon.utils.AppInfoParser) 2021-01-10 12:23:31,333) INFO Kafka startTireN5: 1704889411329 (org.apache.kafka.cornon.utils.AppinfoParser) onolctcd updoting config for topic Corrc lation-Storc. (2U24-8TPIU 12:23-32,052] INFO App Into kafka. adrfn:-fent for adninclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser
 3. Verify that the value changed
kafka-configs.sh --bootstrap-server localhost:9093 -command-config /kafka_2.12-3.3.1/config/producer.properties --entity-type topics --entity-name <TOPIC_NAME> --describe --all I get min.deanable.dirty.ratio
Dash-4.25 katka-contigs.sn --bootstrap-server localhost:9093 --comrana-contig /katka_2.12-3.3.1/config/producer.properties..c gfy. pe topics --entity-nane Correlation-Store --describe --all N grep min.cleanoble.dirty.ratio .clcanable.cirty.ratio-0.1 sensitive-false synonyng-(OYILAMIC_TOPIC_CONFIG:nin.clcanablc.dirty.rat1o=0.1,DEFAULT_COIFIG:log.clean nin. nin.cleanable.ratio=0.5)
 S After the compaction completes, we can set the value back

----

import groovy.transform.Field
eField List<String> topiclistCached = []
 static def _kubectl(String kubect1, String config, String namespace, String cmd) ( return "Sfkubectl) --kubeconfig=$(config) -n $(namespace) exec kafka-broker4-0 -c kafka-broker "bash -C 'KAFKA_LOG4)_OPTS=-Dlog4j.configuration=file://kafka_2.12-3.3.1/config/tools-log4j.properties " "kafka-configs.sh --bootstrap-server 128.168.112.26:9093,128.168.112.27:9093,128.168.112.28:9093,128.179.18.26:9093,128.179.18 "Sfcmd) " I
 static def kafkaTopic(String kubectl, String config, String namespace, String cmd) ( return "S(kubectl) --kubeconfig-sfconfig) -n $(namespace) exec kafka-broker4-0 -c kafka-broker "bash -c 'KAFKA_LOG4)_OPTS--Dlog4j.configuration=file:///kafka_2.12-3.3.1/config/tools-log4j.properties "+ "kafka-topics.sh --bootstrap-server kafka-headless:9092 --command-config /kafka_2.12-3.3.1/config/producer.properties " "S(cmd)" 2>/dev/null"
 2
24 25
 def topicExist (String config, String namespace, String kubectl, String topic) ( if (!topicListCached) ( topiclistCached = sh(script: kafkaTopic(kubectl, config, namespace, '--list'), returnStdout: true) .trim().split("in')
 S
OLG________
def addTopic(String config, String namespace, String kubect1, String topic, String partitions, String policy) f topiclistCached = [] "--create --topic $(topic) --partitions $(partitions) --config cleanup.policy=$fpolicy)" def cmd return sh(script: kafkaTopic(kubectl, config, namespace, cmd), returnStatus: true)
 def setPartition(String config, String namespace, String kubectl, String topic, String partitions) ( def cmd "--alter --topic $ftopic) --partitions $(partitions)" return sh(script: kafkaTopic(kubectl, config, namespace, cmd), returnStatus: true)
 8 39 40 41 42 43 45 46
 def setCleanUp(String config, String namespace, String kubectl, String topic, String policy) ( def cmd = "--alter --entity-type topics --entity-name Sftopic) --add-config cleanup.policy-S(policy)" return sh(script: kafkaTopic(kubectl, config, namespace, cmd), returnStatus: true)
 def handleTopicConfigs(String config, String envToProvision, String kubectl, String topicName, String partitionsNumber, String cleanup printin "Handling topic S(topicNamet, configuring SfpartitionsNumber) partitions and a S(cleanupPolicy) cleanup policy. if (!topicExist(config, envToProvision, kubectl, topicName)) ( addTopic(config, envToProvision, kubectl, topicName, partitionsNumber, cleanupPolicy) )else setPartition(config, envToProvision, kubectl, topicName, partitionsNumber) setCleanUp(config, envToProvision, kubectl, topicName, cleanupPolicy)
 O
DLG
1




kafka-get-offsets --bootstrap-server cit-c.:9092 --topic Throttling-Circuit-Breaker --time -2 command-config /mnt/confiq/client.properties Latest offsets kafka-get-offaeta.sh --bootstrap-server 0.0.0.0:9093 --topic Throttling-Circuit-Breaker --time -l --command-config /kafka_2.12-3.3.1/config/producer.prope. Earliest Offset kafka-get-offaets.sh --bootstrap-server 0.0.0.0:9093 --topic Throttling-Circuit-Breaker time -2 --command-config /kafka_2.12-3.3.1/config/producer.proper. kafka-dump-log.sh --print-data-log --files /tmp/kafka-logs/AMH-EU-Urgent-Throttler-0/00000000000000000010.log
 S 1 6 47 49
 Vormal text file
length:2,313 lines:48
Ln:48 Col:1Sel:0|0
Type here to search
Windows (CR LF) UTF-




// Require the 'session' and 'hm' (headers and metadata) modules
var session = require('session');
var hm = require('header-metadata');

// Declare a variable to hold the parsed JSON payload
var contextVariable;

// Function to read and parse JSON from the request body
session.input.readAsJSON(function(error, jsonData) {
    if (error) {
        // Handle error in reading or parsing JSON
        session.output.write({
            message: "Error reading JSON payload",
            error: error.message
        });
    } else {
        // Save the parsed JSON to a context variable
        contextVariable = jsonData;

        // Log the parsed JSON to check
        console.log("Parsed JSON Data: ", JSON.stringify(contextVariable));

        // Set response headers
        hm.response.statusCode = 200;
        hm.response.set('Content-Type', 'application/json');

        // Send a response back to the client with the parsed data
        session.output.write({
            message: "JSON payload received and saved",
            data: contextVariable
        });
    }
});


----
DR -SCirpt automation

https://github.com/hassanhashmy/kafka-docker-masterr/blob/70d978271db0efd14fc68fd9cb040074dbb4b887/playbooks/reassign-topics-request.json.j2
---

To force push:
--------------
git push origin branchname -f
Eg:
git push origin master -f


To view the remote url
----------
git remote -v 
This will view the git url used


To set new remote url:
-----------
git remote set-url origin urlofthegit
Eg:
git remote set-url origin https://github.com/robin/myapp.git
-----------------

[ req ]
default_bits       = 2048
prompt             = no
default_md         = sha256
distinguished_name = dn
req_extensions     = req_ext

[ dn ]
C                 = US                       # Country
ST                = California                # State
L                 = San Francisco             # Locality/City
O                 = Your Organization Name    # Organization Name
OU                = Your Unit Name            # Organizational Unit Name
CN                = yourdomain.com            # Common Name (CN) (Primary domain)

[ req_ext ]
subjectAltName = @alt_names

[ alt_names ]
DNS.1   = yourdomain.com                      # Primary domain (same as CN)
DNS.2   = www.yourdomain.com                  # Alternate domain (e.g. www)
DNS.3   = api.yourdomain.com                  # Another subdomain



openssl genpkey -algorithm RSA -out private-key.key -pkeyopt rsa_keygen_bits:2048
openssl req -new -key private-key.key -out yourdomain.csr -config openssl-san.cnf

---


// Import the required modules
var urlopen = require('urlopen');

// Set the URL to which the request will be sent
var url = 'https://authuat.emirates.dev/oauth2/ausfs9xvduhmplOrT0h7/v1/token';

// Define the payload for the POST request
var body = 'grant_type=client_credentials&scope=$$$';

// Define the headers
var headers = {
    'Accept': 'application/json',
    'Content-Type': 'application/x-www-form-urlencoded',
    'Authorization': 'Basic ***'  // Replace with your base64 encoded client credentials
};

// Prepare options for the urlopen function
var options = {
    target: url,
    method: 'POST',
    headers: headers,
    data: body
};

// Make the HTTP request
urlopen.open(options, function(error, response) {
    if (error) {
        console.error('Error during request:', error);
        session.output.write('Request failed');
    } else {
        // Read the response
        var responseStatusCode = response.statusCode;
        var responseHeaders = response.headers;
        var responseBody = response.readAsBuffer(function(error, buffer) {
            if (error) {
                console.error('Error reading response body:', error);
            } else {
                session.output.write(buffer.toString());
            }
        });
    }
});


≈==========



#!/bin/bash
sudo echo -e "Tech@2020\nTech@2020" | passwd root;
sudo cp -p /etc/ssh/sshd_config /etc/ssh/sshd_config_backup;
sudo sed -i 's/^PasswordAuthentication.*/PasswordAuthentication yes/' /etc/ssh/sshd_config;
sudo sed -i 's/^PermitRootLogin*/PermitRootLogin yes/' /etc/ssh/sshd_config;
sudo echo "PermitRootLogin yes" >> /etc/ssh/sshd_config;
sudo systemctl restart sshd;
sudo yum install -y wget unzip curl;


================


 sudo -i
 hostnamectl set-hostname gagan-dockerhost
 bash
 
 
 ############# Install Docker - Amazon Linux ##############
 yum install docker
 systemctl start docker
 systemctl enable docker
 docker ps 
 docker run hello-world
 ###############################
 
  ############# Install Docker - Ubuntu ##############
sudo apt-get update -y
sudo apt-get install ca-certificates curl gnupg lsb-release -y
sudo mkdir -p /etc/apt/keyrings


curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o  /etc/apt/keyrings/docker.gpg


echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null


sudo apt-get update -y


sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin -y


#############################################


 ps -ef | grep -i docker
 docker ps
 docker ps -a
 ls
 ls /
 top
 docker run hello-world
 docker ps -a
 docker images
 docker run --name c1  hello-world
 docker ps -a
 docker run --name c1  hello-world   // it should fail due to name conflict
 docker run --name c2 -d  hello-world
 docker ps -a
 docker run --name os-container1 ubuntu   // it should create a exited container because of no terminal
 docker ps -a
 docker run --name c3 nginx
 docker logs c3

docker logs --follow  bce34bb3c617 >> to get the live logs (One of the drawbacks of the above command is that it will contain all the logs from the start)

docker logs --follow --tail 1 bce34bb3c617 (view the continuous log output with the recent records)

docker logs --since 2022-05-16  bce34bb3c617 

docker logs -f containername &> baeldung-postgress.log & 
(we redirect all the live logs to the baeldung-postgress.log file. Also, we run this command in the background using the &, so it will keep running until it is explicitly stopped.)

 docker run --name os-container2 -it  ubuntu   // it'll take you inside the container
  ### Exit for container with exit command
  
 docker ps -a
 docker run --name os-container3 -dt  ubuntu
 docker ps -a
 
 =====================================================
 
  docker ps -a
 docker stop os-container3
 docker ps -a
 docker start os-container3
 docker ps -a
 docker restart os-container3
 docker ps -a
 docker kill os-container3
 docker ps -a
 docker start os-container3
 docker ps -a
 docker rm c1
 docker ps -a
 docker rm os-container3   // it should through error, in case of running containers
 docker rm -f os-container3
 docker ps -a
 docker ps -l >> last container running
 docker ps -lq >> last container running ID
 docker ps -aq >> show all the containers IDs running
 docker ps -q
 docker rm -f ` docker ps -aq`  // Single Postoffice quote near to ESC Button on keyboard
 docker ps -q
 docker ps -a
 docker run -it --name c1 ubuntu:20.04
 docker ps
 docker run -dt --name c2 --hostname c2 ubuntu:20.04
 docker ps
 docker exec -it c2 ls -l / >> execute the command but do not go inside
 docker exec -it c2 ps -ef >> execute the result but do not go inside
 docker exec -it c2 /bin/bash >> will go inside to /bin/bash >> after exit it still runs.
 docker ps
 docker logs c1
 docker top c1
 docker stats
 
 
 ===========================================================
 
 Docker Network:
     
 docker ps
 docker run -dt --name c3 --hostname c3 httpd


 docker ps
 docker exec -it c3 /bin/bash
 ## apt update -y
 ## apt install iproute2 procps curl -y
 ## ip addr
 ## ps -ef
 ## curl localhost
 ## exit
 
 curl 172.17.0.4
 ip addr
 docker ps
 docker inspect c1 | more
 docker inspect c2 | more
 curl 172.17.0.2
 curl 172.17.0.4
 ip route
 docker inspect c3
 ip addr
 docker ps
 docker rm -f c3
 docker run -dt --name c3 --hostname c3 httpd
 docker ps
 docker run -dt --name c4 --hostname c4 -p 80:80  httpd
 docker ps
 docker run -dt --name c5 --hostname c5 -p 80:80  nginx  // it should through error as bounding to existing port
 docker ps -a
 docker run -dt --name c6 --hostname c6 -p 81:80  nginx
 docker run -dt --name c7 --hostname c7 -P  nginx
 docker ps -a
 
 ================================
 NETWORK BRIDGE:
     
ip route
docker network ls
docker inspect c3 | tail -20
docker network inspect 5f668fb6ec99
docker network create gagan-net --subnet 10.0.0.0/16 >> making our network
docker network ls
ip addr
ip route
docker run -dt --name web1 --hostname web1 -P --network gagan-net httpd
docker inspect web1
curl 10.0.0.2
docker ps -a

docker run -dt --network host httpd (will use host network and mapped with the host port as well)
curl localhost 
curl host-ipaddress
curl host-publicIP from the browser

docker network create network1
docker network create network2

docker run -dt --name d3 --network network1 centos
docker run -dt --name d4 --network network1 centos

docker run -dt --name d5 --network network2 centos
docker run -dt --name d6 --network network2 centos

docker network connect network1 d6 (d6 will be able to connect with network1 container via ip and their hostname which is not possible in bridge network)
docker network disconnect network1 d6

docker exec -it 81019715a2cc ping d3
docker exec -it 81019715a2cc ping d4
=====================================
Docker Volume


 docker ps -a
 docker rm -f ` docker ps -aq`
 clear
 docker images
 mkdir /gagan
 fdisk -l
 echo "hello from gagandeep" >> /gagan/index.html
 cat /gagan/index.html
 docker run -dt --name web1 -p 80:80 httpd
 docker exec -it web1 /bin/bash
 docker run -dt --name web2 -p 81:80 -v /gagan:/usr/local/apache2/htdocs  httpd
 cat /gagan/index.html
 echo "Hello from Gagandeep v2" > /gagan/index.html
 docker run -dt --name web3 -p 82:80 -v /gagan:/usr/local/apache2/htdocs  httpd
 docker exec -it web3 /bin/bash
 cd /gagan
 
 ========================================== 
 DOCKER IMAGE
 
 cd /gagan
 vi Dockerfile
 docker images
 docker build -t myimage:latest -f /gagan >> -f to give the location of Dockerfile
 docker images
 docker run -dt --name web1 -p 80:80 myimage
 docker ps -a
 curl localhost
 docker ps
 
 [root@gagan-dockerhost gagan]# cat Dockerfile
FROM centos:7
RUN yum install httpd -y
RUN echo "Hello from gagandeep" > /var/www/html/index.html
RUN touch /gagandeep
CMD ["httpd","-D","FOREGROUND"]
[root@gagan-dockerhost gagan]#


================================================
IMAGE PUSH to DOCKERHUB


 docker images
 docker tag myimage:latest gagandeepthinknyx/myrepo:latest
 docker images
 docker login
 docker push gagandeepthinknyx/myrepo:latest
 docker rm -f ` docker ps -aq`
 docker rmi -f ` docker images -q`
 docker ps -a
 docker images
 docker run -dt --name web1 -p 80:80 gagandeepthinknyx/myrepo:latest
 docker ps
 curl localhost
 
 ============================================


docker commit container-id
docker tag 0c17f0798823 cloudsihmar/myimages:latest
docker push cloudsihmar/myimages:latest

=============================================




